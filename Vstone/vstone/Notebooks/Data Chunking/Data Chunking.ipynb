{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3342a330-ad87-4ac8-983c-aa0c1f534211",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Chunk 1 : used for first time load to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acc82d7b-3543-49ca-9cec-a89a27b04079",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Logic: \n",
    "\n",
    "This section handles the primary transaction data. It performs a \"move\" operation from the raw staging area to a specific chunked directory.\n",
    "\n",
    "####Why this code: \n",
    " \n",
    " We use dbutils.fs.mv instead of a copy to ensure that once files are processed, the source directory is cleared, preventing duplicate processing in future runs. This notebook handles multiple run scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e9eb8ac-cb38-4b27-a535-5d29c32356a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. # 1. Define source and target paths for the first chunk (Transactions)\n",
    "source_path = \"/Volumes/vstone-catalog/vstone_schema/raw_data/transactions\"\n",
    "target_path = \"/Volumes/vstone-catalog/vstone_schema/chunked_data/chunk1/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d304ee5-ad29-4a40-a6ab-ac88310ca1c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 2.# 2. Ensure the destination directory exists to avoid path errors\n",
    "# dbutils.fs.mkdirs is idempotent (won't error if it already exists)\n",
    "dbutils.fs.mkdirs(target_path)\n",
    "\n",
    "# 3. List files in the source and move them to the target\n",
    "try:\n",
    "    files = dbutils.fs.ls(source_path)\n",
    "    \n",
    "    if len(files) == 0:\n",
    "        print(f\"No files found in {source_path}. They may have already been moved.\")\n",
    "    else:\n",
    "        for file in files:\n",
    "            # Construct the full destination path for each file\n",
    "            destination = f\"{target_path}/{file.name}\"\n",
    "            \n",
    "            # 4. Perform the move operation\n",
    "            # Moving is faster than copying and manages source cleanup automatically\n",
    "            print(f\"Moving: {file.name}...\")\n",
    "            dbutils.fs.mv(file.path, destination)\n",
    "            \n",
    "        print(\"Move operation complete.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Logic: If the folder is missing, it's likely because the move was successful previously\n",
    "    if \"java.io.FileNotFoundException\" in str(e):\n",
    "        print(\"Source directory not found. The files have likely already been moved.\")\n",
    "    else:# Re-raise error if it's a different, unexpected issue\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35e15cb3-38fe-4239-82a8-5376b90e6fb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Chunk 2 : Used for Incremental load to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b550f14-f631-40e5-a19f-4f8d019b02a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Logic:\n",
    "\n",
    " This chunk processes transaction_items. Similar to Chunk 1, it utilizes a move logic but is separated to allow for a different scheduling frequency (incremental updates). \n",
    " \n",
    "####Why this code: \n",
    " \n",
    " Separating items from parent transactions allows for more granular data management and troubleshooting if the incremental feed fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5c18cb4-ede7-4744-a20b-07393707964f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Define source and destination paths\n",
    "source_path = \"/Volumes/vstone-catalog/vstone_schema/raw_data/transaction_items\"\n",
    "target_path = \"/Volumes/vstone-catalog/vstone_schema/chunked_data/chunk2/transaction_items\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b43003bf-2ad8-4e45-92f7-bdf9f318b60e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 2. Idempotent directory creation\n",
    "# This won't fail if the directory already exists\n",
    "dbutils.fs.mkdirs(target_path)\n",
    "\n",
    "# 3. Handle the file movement logic with error trapping\n",
    "try:\n",
    "    files = dbutils.fs.ls(source_path)\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"Notice: No files found in {source_path}. They may have already been moved in a previous run.\")\n",
    "    else:\n",
    "        for file in files:\n",
    "            # Construct destination path\n",
    "            destination = f\"{target_path}/{file.name}\"\n",
    "            \n",
    "            # Logic: We use mv to ensure the 'raw_data' folder stays clean for the next delta load\n",
    "            print(f\"Moving: {file.name}\")\n",
    "            dbutils.fs.mv(file.path, destination)\n",
    "            \n",
    "        print(\"Success: All transaction_items moved to chunk2.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Catching the error if the source folder was deleted/moved entirely\n",
    "    if \"FileNotFoundException\" in str(e):\n",
    "        print(\"Source path not found. Check if files were already moved to chunk2.\")\n",
    "    else:\n",
    "        print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c09ca071-b555-4404-b5d1-b13a1bc05e35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Chunk 3 : Used for Json Format Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "062bfb51-1580-4630-bd48-c4e9f176a142",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Logic: \n",
    "\n",
    "This section converts standard CSV user data into JSON format. \n",
    "\n",
    "####Why this code:\n",
    "\n",
    " Downstream applications (like web services or NoSQL databases) often require JSON. We use spark.read and df.write.json to handle the schema conversion automatically, and then use dbutils to \"flatten\" the Spark output from a folder into a single clean .json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ee8368f-d106-45b9-8d73-ea4c142fd408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of folders containing data that requires JSON formatting\n",
    "folders = [\n",
    "     \"users\"\n",
    "]\n",
    "\n",
    "base_raw_path = \"/Volumes/vstone-catalog/vstone_schema/raw_data\"\n",
    "base_target_path = \"/Volumes/vstone-catalog/vstone_schema/chunked_data/chunk3/users/\"\n",
    "\n",
    "# Ensure the main output directory exists\n",
    "dbutils.fs.mkdirs(base_target_path)\n",
    "\n",
    "for folder in folders:\n",
    "    source_dir = f\"{base_raw_path}/{folder}\"\n",
    "    \n",
    "    # Logic: Filter out directories or hidden Spark metadata files (starting with _)\n",
    "    try:\n",
    "        files = [f for f in dbutils.fs.ls(source_dir) if not f.isDir() and not f.name.startswith(\"_\")]\n",
    "        \n",
    "        print(f\"--- Processing {len(files)} files in folder: {folder} ---\")\n",
    "        \n",
    "        for file in files:\n",
    "            # Create a unique name: foldername_filename.json\n",
    "            file_name_clean = file.name.split('.')[0]\n",
    "            final_name = f\"{file_name_clean}.json\"\n",
    "            final_path = f\"{base_target_path}/{final_name}\"\n",
    "            \n",
    "            # Logic: Spark writes to a directory. We use a temp_dir to hold the Spark output.\n",
    "            temp_dir = f\"{base_target_path}/_temp_{final_name}\"\n",
    "            \n",
    "            # 1. Read CSV source\n",
    "            df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file.path)\n",
    "            \n",
    "            # 2. Write to JSON format\n",
    "            # coalesce(1) ensures we only get one part file\n",
    "            df.coalesce(1).write.mode(\"overwrite\").json(temp_dir)\n",
    "            \n",
    "           # 3. Rename/Extract: Spark names the file 'part-000...'. \n",
    "            # We copy it out to our 'final_path' with the correct name.\n",
    "            part_file = [f for f in dbutils.fs.ls(temp_dir) if f.name.startswith(\"part-\")][0]\n",
    "            dbutils.fs.cp(part_file.path, final_path)\n",
    "            \n",
    "            # 4. Cleanup: Delete the temporary Spark folder to keep the workspace clean\n",
    "            dbutils.fs.rm(temp_dir, recurse=True)\n",
    "            \n",
    "            print(f\"Created: {final_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {folder}: {e}\")\n",
    "\n",
    "print(\"\\nConversion complete. Every raw file now has a matching JSON file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89c949cb-b98e-413c-8acb-8ad63e216bd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Chunk 4 : Used for XML Format Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fc0669d-ca70-4294-8385-97878e48178f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "####Logic: \n",
    "\n",
    "This chunk processes reference data (vouchers, stores, etc.) into XML. \n",
    "\n",
    "####Why this code: \n",
    "\n",
    "Many legacy systems or third-party integrations require XML. This script uses the Spark-XML connector logic to wrap CSV rows into specific rootTag and rowTag structures, ensuring the data is valid for XML parsers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d37136a-f0ad-472b-8bd0-bdee40bd896e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Configuration for reference data tables\n",
    "folders_to_convert = [\"vouchers\", \"payment_methods\", \"menu_items\", \"stores\"]\n",
    "base_raw_path = \"/Volumes/vstone-catalog/vstone_schema/raw_data/\"\n",
    "target_chunk4 = \"/Volumes/vstone-catalog/vstone_schema/chunked_data/chunk4/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0b94c06-9bf6-4e38-9548-a1817bf4d954",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Ensure target directory exists\n",
    "dbutils.fs.mkdirs(target_chunk4)\n",
    "\n",
    "for folder in folders_to_convert:\n",
    "    source_dir = f\"{base_raw_path}/{folder}\"\n",
    "    \n",
    "    try:\n",
    "        # Get all individual files (ignoring directories and hidden/metadata files)\n",
    "        files = [f for f in dbutils.fs.ls(source_dir) if not f.isDir() and not f.name.startswith(\"_\")]\n",
    "        \n",
    "        print(f\"--- Converting {len(files)} files from {folder} ---\")\n",
    "        \n",
    "        for file in files:\n",
    "            # Create a clean name for the output\n",
    "            file_name_base = file.name.split('.')[0]\n",
    "            final_xml_name = f\"{file_name_base}.xml\"\n",
    "            final_path = f\"{target_chunk4}/{final_xml_name}\"\n",
    "            \n",
    "            # Temporary folder for Spark output\n",
    "            temp_dir = f\"{target_chunk4}/_temp_{final_xml_name}\"\n",
    "            \n",
    "           # 2. Read file into a Spark DataFrame\n",
    "            df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file.path)\n",
    "            \n",
    "            # 3. Write to XML (This creates a folder with a part-file inside)\n",
    "            # 'rootTag' and 'rowTag' define the XML structure\n",
    "            df.coalesce(1).write.format(\"xml\") \\\n",
    "                .option(\"rootTag\", f\"{folder}_data\") \\\n",
    "                .option(\"rowTag\", \"item\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .save(temp_dir)\n",
    "            \n",
    "            # 4. Extract the XML part-file and rename it to the final destination\n",
    "            # This ensures we get a single .xml file, not a folder\n",
    "            actual_xml_part = [f for f in dbutils.fs.ls(temp_dir) if f.name.startswith(\"part-\") and f.name.endswith(\".xml\")][0]\n",
    "            dbutils.fs.cp(actual_xml_part.path, final_path)\n",
    "            \n",
    "            # 5. Remove Spark's temporary folder and metadata\n",
    "            dbutils.fs.rm(temp_dir, recurse=True)\n",
    "            \n",
    "            print(f\"Successfully created: {final_xml_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing folder {folder}: {e}\")\n",
    "\n",
    "print(f\"\\nTask Complete! XML files are located in: {target_chunk4}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Chunking",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
