{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2a0b296-5588-425f-9acf-3481a3c4e8c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1. Environment Configuration via Widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15e58c01-56ab-4d1a-8db4-20850832174d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Logic: \n",
    "\n",
    "The script uses dbutils.widgets to retrieve the catalog name, schema name, source path, and checkpoint location. \n",
    "\n",
    "####Why this code: \n",
    "\n",
    "Using widgets allows the notebook to be generic and portable. The same code can run in Development, Staging, or Production environments simply by passing different values from the DAB variables.yml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bf49e0f-22db-4de3-9e0c-77e12910f9bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "# 1. Capture Environment Variables via Widgets\n",
    "# Logic: Text widgets act as input parameters for the notebook\n",
    "dbutils.widgets.text(\"catalog_name\", \"\")\n",
    "dbutils.widgets.text(\"schema_name\", \"\")\n",
    "dbutils.widgets.text(\"source_path\", \"\")\n",
    "dbutils.widgets.text(\"checkpoint_base\", \"\") # New: Passed from DAB variables.yml\n",
    "\n",
    "# 2. Assign widget values to Python variables for downstream use\n",
    "catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
    "schema_name = dbutils.widgets.get(\"schema_name\")\n",
    "source_path = dbutils.widgets.get(\"source_path\")\n",
    "checkpoint_base = dbutils.widgets.get(\"checkpoint_base\")\n",
    "\n",
    "# Construct the full Unity Catalog table identifier\n",
    "target_table = f\"`{catalog_name}`.`{schema_name}`.`users_bronze`\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22675996-ce5c-4005-ad4d-76cd49781fa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Streaming Ingestion with Auto Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "870fa992-2ccb-4ba1-969f-f78e9debd355",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "This block performs the actual data movement using Spark Structured Streaming.\n",
    "\n",
    "####Logic: \n",
    "\n",
    "The code uses spark.readStream with the cloudFiles format to automatically detect new JSON files in the source directory. It includes schema inference and evolution support. \n",
    "\n",
    "####Why this code: \n",
    "\n",
    "* Auto Loader (cloudFiles): Efficiently processes millions of files and handles schema drift (new columns) automatically.\n",
    "\n",
    "* Checkpointing: Stores the state of the stream in checkpoint_base, ensuring that if the job fails, it resumes exactly where it left off without reprocessing data.\n",
    "\n",
    "* Trigger availableNow=True: Processes all available data as a batch-like stream, which is cost-effective for scheduled jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6536d858-7567-4378-a822-5a7ef2a5f54d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 3. Ensure the target Bronze table exists before starting the stream\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS {target_table}\")\n",
    "# 4. Define and start the Auto Loader streaming query\n",
    "query = (\n",
    "    spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"json\")\n",
    "        # Logic: Auto-detect data types and store the inferred schema\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\") \n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_base}/schema\")\n",
    "        .load(source_path)\n",
    "        # Logic: Add audit columns to track load time and source origin\n",
    "        .withColumn(\"load_dt\", current_timestamp())\n",
    "        .withColumn(\"source\", lit(\"dab_json_ingestion\"))\n",
    "        .writeStream\n",
    "        .format(\"delta\")\n",
    "        # Logic: Update the Delta table schema if new fields appear in the JSON\n",
    "        .option(\"mergeSchema\", \"true\") \n",
    "        .option(\"checkpointLocation\", checkpoint_base)\n",
    "        .outputMode(\"append\")\n",
    "        # Logic: Process only new data since the last run, then shut down\n",
    "        .trigger(availableNow=True)\n",
    "        .table(target_table)\n",
    ")\n",
    "# Logic: Block the cell until all data is processed\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Chunk 3 Ingestion using Autoloader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
