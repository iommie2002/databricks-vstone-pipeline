{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a53ae5d-d219-4dc2-ad6c-b4cb1f40f78e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook follows the Medallion Architecture to transform raw voucher and discount data from the Bronze layer into a high-quality, validated Silver table. The process focuses on cleaning date formats, standardizing discount values, and implementing data quality gates to ensure that only valid, usable vouchers are available for the Gold layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ef780e7-ed20-46d9-a92c-7f0f86a3e922",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Silver Layer: Vouchers Transformation\n",
    "**Notebook Objective:** \n",
    "The notebook implements the refining of promotional voucher data. It ensures that every voucher has a valid identifier and discount value, standardizes expiration dates for time-sensitive analysis, and redirects malformed records to a quarantine table for auditing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37ea583a-3374-4cc1-9438-8331d480a991",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1. Initial Data Profiling (Bronze Layer)\n",
    "We start by performing a \"health check\" on the raw data to identify missing identifiers or illogical discount values (such as negative numbers) that would disrupt financial reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "631eba83-c252-48a7-9f07-02b583f4f5f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 1. Identify \"Logical Date Errors\" (Start date after End date)\n",
    "SELECT count(*) \n",
    "FROM `vstone-catalog`.bronze_schema.bronze_vouchers \n",
    "WHERE valid_from > valid_to;\n",
    "\n",
    "-- 2. Validate financial logic: Discounts must be greater than 0\n",
    "SELECT count(*) \n",
    "FROM `vstone-catalog`.bronze_schema.bronze_vouchers \n",
    "WHERE discount_value > 100 OR discount_value < 0;\n",
    "\n",
    "-- 3. Check for Voucher Code uniqueness\n",
    "SELECT voucher_code, count(*) \n",
    "FROM `vstone-catalog`.bronze_schema.bronze_vouchers \n",
    "GROUP BY voucher_code HAVING count(*) > 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5094c243-a074-4513-b595-27d0ed4e201e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Configuration & Environment Setup\n",
    "We define the naming conventions for our Delta tables using Unity Catalog standards and initialize the destination schema to ensure a self-contained pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "063dc16b-ebdd-4d01-8d8a-a08c2fea4a22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "# Using backticks to handle hyphens in the catalog name for Spark SQL compliance\n",
    "CATALOG = \"`vstone-catalog`\"\n",
    "SILVER_SCHEMA = \"silver_schema\"\n",
    "BRONZE_TABLE = f\"{CATALOG}.bronze_schema.bronze_vouchers\"\n",
    "SILVER_TABLE = f\"{CATALOG}.{SILVER_SCHEMA}.silver_vouchers\"\n",
    "QUARANTINE_TABLE = f\"{CATALOG}.{SILVER_SCHEMA}.quarantine_vouchers\"\n",
    "\n",
    "# Bootstrap: Ensure the Silver schema exists before processing--\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SILVER_SCHEMA}\")\n",
    "\n",
    "# --- 2. PANDAS UDF FOR STANDARDIZATION ---\n",
    "@pandas_udf(StringType())\n",
    "def clean_voucher_code_udf(code_series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Removes whitespace and forces uppercase for voucher codes.\"\"\"\n",
    "    return code_series.str.strip().str.upper()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "087c5fd9-1fb7-4f80-83a8-132cb46a90d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Data Ingestion & Header Standardization\n",
    "To maintain a clean and searchable Data Lakehouse, we standardize all column headers into snake_case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27a42080-7ce2-4686-aa1d-21fd39ff2597",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- 3. LOAD & STANDARDIZE HEADERS ---\n",
    "df_bronze = spark.read.table(BRONZE_TABLE)\n",
    "\n",
    "# Convert all column names to lowercase and replace spaces with underscores\n",
    "standardized_cols = [col.lower().replace(\" \", \"_\").strip() for col in df_bronze.columns]\n",
    "df_standardized = df_bronze.toDF(*standardized_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1eeeab89-1769-4e13-b0fb-38f632981a75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##4. Quality Gates & Quarantine Logic\n",
    "In professional data engineering, we never discard data. Instead, we use a **Quarantine Pattern** to divert records that fail business rules (like missing IDs or invalid discount amounts) for later review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b9da936-a6a6-4ab1-99f7-3477bc8f5e41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- 3. QUALITY GATES ---\n",
    "# Business Rules: \n",
    "# 1. voucher_id must exist.\n",
    "# 2. discount_value must be positive\n",
    "df_prepared = df_standardized.withColumn(\"discount_value\", F.col(\"discount_value\").cast(\"double\")) \\\n",
    "                             .withColumn(\"valid_from\", F.to_date(\"valid_from\")) \\\n",
    "                             .withColumn(\"valid_to\", F.to_date(\"valid_to\"))\n",
    "\n",
    "# Business Rules:\n",
    "date_logic_valid = (F.col(\"valid_from\") <= F.col(\"valid_to\"))\n",
    "id_valid = (F.col(\"voucher_id\").isNotNull()) & (F.col(\"voucher_code\").isNotNull())\n",
    "value_valid = (F.col(\"discount_value\") >= 0)\n",
    "\n",
    "valid_mask = date_logic_valid & id_valid & value_valid\n",
    "\n",
    "# Redirect failed records to a Quarantine table with a reason code\n",
    "df_quarantine = df_prepared.filter(~valid_mask) \\\n",
    "    .withColumn(\"quarantine_reason\", \n",
    "        F.when(~id_valid, \"MISSING_ID_OR_CODE\")\n",
    "         .when(~date_logic_valid, \"DATE_LOGIC_ERROR\")\n",
    "         .otherwise(\"INVALID_DISCOUNT_VALUE\")) \\\n",
    "    .withColumn(\"quarantined_at\", F.current_timestamp())\n",
    "\n",
    "# Proceed with clean data only\n",
    "df_clean = df_prepared.filter(valid_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b573a27f-442d-45d5-a9a7-ee0a882cf0d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##5. Deduplication & Final Transformation\n",
    "Vouchers may have multiple updates in the raw system. We use a Window function to ensure each voucher_id is unique in Silver, keeping only the most recent entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1de90339-b474-4f2e-bfb6-49caf3aea4aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- 4. DEDUPLICATION & NORMALIZATION ---\n",
    "# Logic: Partition by voucher_id and keep the latest record based on load_dt\n",
    "window_spec = Window.partitionBy(\"voucher_id\").orderBy(F.col(\"load_dt\").desc())\n",
    "\n",
    "df_silver_final = df_clean.withColumn(\"row_rank\", F.row_number().over(window_spec)) \\\n",
    "    .filter(\"row_rank == 1\") \\\n",
    "    .drop(\"row_rank\") \\\n",
    "    .withColumn(\"voucher_code\", clean_voucher_code_udf(F.col(\"voucher_code\"))) \\\n",
    "    .withColumn(\"discount_value\", F.round(F.col(\"discount_value\"), 2)) \\\n",
    "    .withColumn(\"load_dt\", F.to_timestamp(F.col(\"load_dt\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b66e4cc9-8e7f-4fbc-96cc-55ddd6d8fd10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6. Atomic Delta Writes & Constraints\n",
    "The data is committed using the Delta Lake format. We apply storage-level constraints to act as a \"firewall,\" ensuring that future data writes cannot violate our core integrity rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90e02e00-15f4-451e-a1b3-952494924144",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- 5. ATOMIC WRITES ---\n",
    "# Write to Quarantine (Append) and Silver (Overwrite)\n",
    "df_quarantine.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(QUARANTINE_TABLE)\n",
    "\n",
    "df_silver_final.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(SILVER_TABLE)\n",
    "\n",
    "# --- 6. APPLY DELTA CONSTRAINTS ---\n",
    "# Ensure primary keys are NOT NULL and business logic is enforced at the storage layer\n",
    "spark.sql(f\"ALTER TABLE {SILVER_TABLE} ALTER COLUMN voucher_id SET NOT NULL\")\n",
    "\n",
    "try:\n",
    "    spark.sql(f\"ALTER TABLE {SILVER_TABLE} ADD CONSTRAINT valid_date_range CHECK (valid_from <= valid_to)\")\n",
    "except Exception as e:\n",
    "    print(f\"Constraint valid_date_range skipped: {e}\")\n",
    "\n",
    "print(f\"Silver table {SILVER_TABLE} updated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "395b9341-f222-4a8a-be4b-3eeb809ccb39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Identify the version before the last update\n",
    "DESCRIBE HISTORY `vstone-catalog`.silver_schema.silver_vouchers;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6922d307-bb22-49bd-89a9-722179a86bee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Query version 5 to see old voucher values\n",
    "SELECT * FROM `vstone-catalog`.silver_schema.silver_vouchers VERSION AS OF 3\n",
    "WHERE voucher_id = 101;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a0bb787-c190-4a94-85dd-f292de054944",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Industry Logics & Standards \n",
    "**Financial Accuracy:** By casting discount_value to a double and rounding to two decimal places, we ensure the data is ready for precise financial reporting in the Gold layer.\n",
    "\n",
    "**Idempotency:** The notebook uses .mode(\"overwrite\"), meaning it can be re-run multiple times without creating duplicate data or inflating record counts.\n",
    "\n",
    "**Auditability:** The Quarantine table provides a full audit trail of \"bad data.\" This allows data engineers to identify if a specific source system is consistently sending incorrect voucher codes.\n",
    "\n",
    "**Temporal Precision:** Standardizing expiry_date to a date type and load_dt to a timestamp is critical for point-in-time analysis, such as \"How many vouchers were valid on January 1st?\""
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5480252379363769,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Vouchers Silver Table Creation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
