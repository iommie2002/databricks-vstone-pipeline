{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee53a6e0-47f2-48a5-9c5f-c648742edc0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook follows the Medallion Architecture to transform raw payment method data into a structured, high-quality Silver table. It focuses on data profiling, header standardization, deduplication, and the implementation of a \"Quarantine\" pattern for data governance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "399c9eb9-a9d6-4175-9260-a02d595d08f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Silver Layer: Payment Methods Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f81bc47c-dd99-4d8c-ae93-9f9fad2be108",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Notebook Objective:** This notebook implements the cleaning and validation of payment method data. It ensures that the final dataset is deduplicated, follows naming standards, and adheres to strict business rules (Data Quality Gates) before being used for financial reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c24924fc-d07d-4158-a8c9-f6038fd561c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1. Initial Data Profiling (Bronze Layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cff3ec32-8e6e-40e9-86ff-7a2a2c8101aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Before processing, we perform \"smoke tests\" using SQL to understand the state of the raw data. This helps identify null values, duplicates, and category distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d426d28a-4f64-442d-8bb5-2406a517e529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 1. Check for Nulls in critical columns to determine if a quarantine is needed\n",
    "SELECT \n",
    "  count(*) - count(method_id) AS missing_ids,\n",
    "  count(*) - count(method_name) AS missing_names\n",
    "FROM `vstone-catalog`.bronze_schema.bronze_paymentmethods;\n",
    "\n",
    "-- 2. Check for Duplicate method_ids; Silver requires a unique grain per ID\n",
    "SELECT method_id, count(*) \n",
    "FROM `vstone-catalog`.bronze_schema.bronze_paymentmethods \n",
    "GROUP BY method_id HAVING count(*) > 1;\n",
    "\n",
    "-- 3. Check for Category distribution to ensure data completeness\n",
    "\n",
    "SELECT category, count(*) \n",
    "FROM `vstone-catalog`.bronze_schema.bronze_paymentmethods \n",
    "GROUP BY category;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f6386a0-5380-43dd-bb4c-8e18e2c7408e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Configuration & Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afecdc1d-8dc0-4cca-89cf-63c02b3dacb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We define our environment variables and implement a Pandas UDF (User Defined Function). UDFs are used here to ensure that string formatting is handled efficiently across the distributed Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20efa4d1-144c-43c0-8235-8de83987599c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "# Note: Backticks are required because of the hyphen in the catalog name\n",
    "CATALOG = \"`vstone-catalog`\"\n",
    "SILVER_SCHEMA = \"silver_schema\"\n",
    "BRONZE_TABLE = f\"{CATALOG}.bronze_schema.bronze_paymentmethods\"\n",
    "SILVER_TABLE = f\"{CATALOG}.{SILVER_SCHEMA}.silver_paymentmethods\"\n",
    "QUARANTINE_TABLE = f\"{CATALOG}.{SILVER_SCHEMA}.quarantine_paymentmethods\"\n",
    "\n",
    "# Initialize Environment\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SILVER_SCHEMA}\")\n",
    "\n",
    "# --- 2. PANDAS UDF FOR COLUMN STANDARDIZATION ---\n",
    "@pandas_udf(StringType())\n",
    "def standardize_header_udf(col_series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Standardizes string inputs to lowercase snake_case.\"\"\"\n",
    "    return col_series.str.lower().str.replace(r'[^a-zA-Z0-9]', '_', regex=True).str.strip('_')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddc223e2-3029-42a0-8141-0749e200b411",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Data Cleaning & Header Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5df1c249-1ec2-41f4-8c4a-8d7767a4ff5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Industry standards dictate that column names should be consistent (e.g., no spaces or hyphens). We rename headers to snake_case to ensure compatibility across various BI tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5658598e-68e3-4c63-88fb-6a79cbd958a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 3. LOAD & STANDARDIZE HEADERS ---\n",
    "df_bronze = spark.read.table(BRONZE_TABLE)\n",
    "\n",
    "# Apply standardization to the column names themselves\n",
    "# We use Python logic here for column names, while the UDF is better for row data\n",
    "standardized_col_names = [\n",
    "    col.lower().strip().replace(\" \", \"_\").replace(\"-\", \"_\") \n",
    "    for col in df_bronze.columns\n",
    "]\n",
    "df_standardized = df_bronze.toDF(*standardized_col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef83c6b5-6c08-4f3b-8fc4-1e009ec6d3aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##4. Quality Gates & Quarantine Pattern\n",
    "To prevent the code from failing due to bad data, we use a Quarantine Pattern. Invalid records (missing IDs) are redirected to a separate table for auditing, while clean records proceed to the Silver table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2f5a14c-465a-4468-a564-46655c808e2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 4. QUALITY GATES & QUARANTINE ---\n",
    "# Business Rule: method_id is our primary key and must NOT be null\n",
    "is_valid = F.col(\"method_id\").isNotNull()\n",
    "\n",
    "# Isolate malformed records for the Data Quality team to review\n",
    "df_quarantine = df_standardized.filter(~is_valid) \\\n",
    "    .withColumn(\"quarantine_reason\", F.lit(\"MISSING_METHOD_ID\")) \\\n",
    "    .withColumn(\"quarantined_at\", F.current_timestamp())\n",
    "\n",
    "# Filter only clean data for the Silver table\n",
    "df_clean = df_standardized.filter(is_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6aa74b5b-03b5-45f5-9e15-251e94a07adb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##5. Deduplication & Final Transformation\n",
    "We use a Window Function to handle duplicates. By ranking records by their load timestamp, we ensure that only the \"latest version\" of a payment method is kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43453881-be03-4ab9-9635-27135695a041",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 5. DEDUPLICATION & NORMALIZATION ---\n",
    "# Logic: Partition by ID and keep the most recent record (row_rank == 1)\n",
    "window_spec = Window.partitionBy(\"method_id\").orderBy(F.col(\"load_dt\").desc())\n",
    "\n",
    "df_silver_final = df_clean.withColumn(\"row_rank\", F.row_number().over(window_spec)) \\\n",
    "    .filter(\"row_rank == 1\") \\\n",
    "    .drop(\"row_rank\") \\\n",
    "    .withColumn(\"load_dt\", F.to_timestamp(F.col(\"load_dt\"))) \\\n",
    "    .withColumn(\"method_name\", F.initcap(F.col(\"method_name\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14428e15-f29d-49a7-9c6d-810a8c82763e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6. Atomic Delta Writes & Constraints\n",
    "We commit the data using **Delta Lake.** We also apply an ALTER TABLE constraint. This acts as a permanent \"firewall\" to ensure no future null IDs can ever be written into the Silver layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23d63204-4dad-4f35-babe-b8744848eac4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- 6. ATOMIC WRITES ---\n",
    "# Append failed records to the audit log\n",
    "df_quarantine.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(QUARANTINE_TABLE)\n",
    "\n",
    "# Overwrite Silver table with clean, deduplicated data\n",
    "df_silver_final.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(SILVER_TABLE)\n",
    "\n",
    "# --- 7. APPLY DELTA CONSTRAINTS ---\n",
    "# Enforce NOT NULL at the storage level for absolute data integrity\n",
    "spark.sql(f\"ALTER TABLE {SILVER_TABLE} CHANGE COLUMN method_id SET NOT NULL\")\n",
    "\n",
    "print(f\"Success: {SILVER_TABLE} and {QUARANTINE_TABLE} have been updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8cc3c08-9b1c-49af-8cae-631171dd1efe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY `vstone-catalog`.silver_schema.silver_paymentmethods;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72eece2a-6fb6-416f-bbbb-e2d4ab38f5af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Query using the earliest available timestamp\n",
    "SELECT * FROM `vstone-catalog`.silver_schema.silver_paymentmethods \n",
    "TIMESTAMP AS OF '2026-01-12 16:14:50';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ad6486d-2eb3-4e95-983c-e9eb93a0457e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- This should return 0 rows thanks to our filter and Delta constraints\n",
    "SELECT * FROM `vstone-catalog`.`silver_schema`.silver_paymentmethods WHERE method_id IS NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19a1000c-87c1-42b3-8a07-65ed83a10dfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Industry Logics & Standards \n",
    "**1. The \"Single Version of Truth\"**\n",
    "In the Bronze layer, data might be duplicated because of multiple system exports. In the Silver layer, the logic partitionBy(\"method_id\").orderBy(desc(\"load_dt\")) ensures that for every ID, only the most current record exists. This is critical for financial accuracy.\n",
    "\n",
    "**2. Schema Evolution vs. Enforcement**\n",
    "We use .option(\"overwriteSchema\", \"true\"). In an industry setting, this allows the Silver table to adapt if new columns are added to the source system, while the SET NOT NULL constraint ensures that even if the schema grows, the quality of the primary keys never degrades.\n",
    "\n",
    "**3. Time Travel Capability**\n",
    "The notebook includes a command for TIMESTAMP AS OF. Because this is a Delta Table, we can query the state of the payment methods as they existed at a specific point in time. This is standard for auditing and \"undoing\" accidental data deletions.\n",
    "\n",
    "**4. Data Governance (Quarantine)**\n",
    "By tagging records with a quarantine_reason, we transform a \"data failure\" into \"actionable metadata.\" Instead of the pipeline crashing, the data engineer receives an alert to check the quarantine_paymentmethods table to fix the source system."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8753912885143924,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Paymentmethods Silver table Creation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
