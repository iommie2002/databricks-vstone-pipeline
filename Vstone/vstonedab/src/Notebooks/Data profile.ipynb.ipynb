{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24df28db-eefb-45ac-87f7-bfe3803080f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1. Core Profiling Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6905cc60-5b3d-4e59-b104-776abb3735d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This section defines the basic_profile function, which serves as the primary engine for analyzing any Spark DataFrame.\n",
    "\n",
    "####Logic: \n",
    "\n",
    "It programmatically iterates through every column to count nulls and calculates distinctness based on provided Primary Key (PK) columns. \n",
    "\n",
    "####Why this code: \n",
    "\n",
    "Manual profiling is time-consuming. This function centralizes logic to ensure consistent data quality checks across the entire pipeline, including date range validation for temporal datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fde4995c-81b4-406d-b925-1c21f1c16416",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import essential PySpark functions for aggregation and column manipulation\n",
    "from pyspark.sql.functions import col, count, when, min, max\n",
    "from pyspark.sql.functions import col, sum as spark_sum, min, max, lit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b568d0ff-75ca-4b5e-a07e-6ddddf96eb4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def basic_profile(df, table_name, pk_cols=None, date_col=None):\n",
    "    total_rows = df.count()\n",
    "\n",
    "    # -------------------------\n",
    "    # # 1. Null counts per column\n",
    "    # Logic: Cast isNull() booleans to integers (1 or 0) and sum them up for every column.\n",
    "    # -------------------------\n",
    "    null_counts = df.select([\n",
    "        spark_sum(col(c).isNull().cast(\"int\")).alias(c)\n",
    "        for c in df.columns\n",
    "    ])\n",
    "\n",
    "    # -------------------------\n",
    "    # # 2. Duplicate detection\n",
    "    # Logic: Subtract the count of distinct PK combinations from the total row count.\n",
    "    # -------------------------\n",
    "    duplicate_rows = 0\n",
    "    if pk_cols:\n",
    "        duplicate_rows = total_rows - df.select(pk_cols).distinct().count()\n",
    "\n",
    "    # -------------------------\n",
    "    # 3. Date range analysis\n",
    "    # Logic: Identify the minimum and maximum dates to verify data freshness/coverage.\n",
    "    # -------------------------\n",
    "    min_date = None\n",
    "    max_date = None\n",
    "    if date_col:\n",
    "        dates = df.select(\n",
    "            min(col(date_col)).alias(\"min_date\"),\n",
    "            max(col(date_col)).alias(\"max_date\")\n",
    "        ).collect()[0]\n",
    "\n",
    "        min_date = dates[\"min_date\"]\n",
    "        max_date = dates[\"max_date\"]\n",
    "\n",
    "   # -------------------------\n",
    "    # 4. Final summary construction\n",
    "    # Logic: Append the metadata (table name, row counts) to the null count results.\n",
    "    # -------------------------\n",
    "    profile_df = (\n",
    "        null_counts\n",
    "        .withColumn(\"table_name\", lit(table_name))\n",
    "        .withColumn(\"total_rows\", lit(total_rows))\n",
    "        .withColumn(\"duplicate_rows\", lit(duplicate_rows))\n",
    "        .withColumn(\"min_date\", lit(min_date))\n",
    "        .withColumn(\"max_date\", lit(max_date))\n",
    "    )\n",
    "    # Display the final health report in the Databricks UI\n",
    "    display(profile_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae2151ec-df74-4e17-b853-8be3210af6b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Transaction Data Profiling (CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8140cbc-ef8c-4918-baed-a349a37d511a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Logic:\n",
    "\n",
    " This block loads transaction headers and line items from the chunk1 and chunk2 directories. \n",
    " \n",
    "####Why this code: \n",
    " \n",
    " Transaction data is usually the largest. Profiling it helps identify if mandatory fields (like transaction_id) have missing values or if there are integrity issues like duplicate IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad19e587-cf57-42f8-a6e3-03e9bb9174c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load Transaction Header data\n",
    "transactions_df = spark.read.option(\"header\", True).csv(\n",
    "    \"/Volumes/vstone-catalog/vstone_schema/chunked_data/chunk1/\"\n",
    ")\n",
    "# Run health check for Transactions\n",
    "basic_profile(\n",
    "    df=transactions_df,\n",
    "    table_name=\"transactions\",\n",
    "    pk_cols=[\"transaction_id\"],\n",
    "    date_col=\"created_at\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdd44b80-f6bb-462b-83b2-f0b309a621df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\", \"pyspark.sql.classic.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"CmRpc3BsYXkodHJhbnNhY3Rpb25zX2RmLmRlc2NyaWJlKCkp\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "Data Profile 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 1768239284700,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": [],
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "8415a8ff-82e7-435f-92ac-df0ac4fc76be",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 4.5,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 1768239283072,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 1768239243923,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "display(transactions_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fa9df48-38b0-42cf-b0ef-fad94e721b33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load and Profile Transaction Items (using inferSchema to handle numeric prices)\n",
    "transaction_items_df = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\n",
    "        \"/Volumes/vstone-catalog/vstone_schema/chunked_data/chunk2/transaction_items/*.csv\"\n",
    "    )\n",
    ")\n",
    "\n",
    "basic_profile(\n",
    "    df=transaction_items_df,\n",
    "    table_name=\"transaction_items\",\n",
    "    pk_cols=[\"transaction_id\", \"item_id\"],\n",
    "    date_col=\"created_at\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd47b71e-aeef-4871-9eeb-3e40a35a2d27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transaction_items_df.describe().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "270a7a24-b113-48dc-876b-01ecc194d362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. User & Reference Data Profiling (JSON/XML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4675116-3d69-4914-a30e-4e358042e52e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Logic: \n",
    "\n",
    "Analyzes User data (JSON) and static reference tables like Vouchers and Stores (XML). \n",
    "\n",
    "####Why this code: \n",
    "\n",
    "Reference data often has different formats. Profiling XML/JSON ensures that the Spark xml and json readers are correctly parsing the nested or tagged structures into flat DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5916ab8d-3e55-4002-b702-3c7e3133c95d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load User data from JSON\n",
    "users_df = spark.read.json(\n",
    "    \"/Volumes/vstone-catalog/vstone_schema/chunked_data/chunk3/users/\"\n",
    ")\n",
    "# Profile Users (checking for duplicate user IDs and registration date ranges)\n",
    "basic_profile(\n",
    "    df=users_df,\n",
    "    table_name=\"users\",\n",
    "    pk_cols=[\"user_id\"],\n",
    "    date_col=\"registered_at\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8720168-b308-4d28-988e-7b37e6eaaa1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "users_df.describe().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e48c105c-6c42-4a87-9b47-f278dc910573",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load Voucher data from XML using 'item' as the row delimiter\n",
    "vouchers_df = (\n",
    "    spark.read\n",
    "    .format(\"xml\")\n",
    "    .option(\"rowTag\", \"item\")\n",
    "    .load(\"/Volumes/vstone-catalog/vstone_schema/chunked_data/chunk4/vouchers.xml\")\n",
    ")\n",
    "# Profile Vouchers (verifying total discount entries and potential duplicates)\n",
    "basic_profile(\n",
    "    df=vouchers_df,\n",
    "    table_name=\"vouchers\",\n",
    "    pk_cols=[\"voucher_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b565bde8-8234-4f5e-8e59-f342c3f5ad6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\", \"pyspark.sql.classic.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheSh2b3VjaGVyc19kZi5kZXNjcmliZSgpKQ==\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "Data Profile 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 1768239284700,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "6e3a62fc-f086-4c9c-af84-e866b1285e5e",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 9.5,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 1768239284700,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 1768239243949,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(vouchers_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ef4af0b-782f-40c8-9f85-786e492fe4be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "menu_items_df = (\n",
    "    spark.read\n",
    "    .format(\"xml\")\n",
    "    .option(\"rowTag\", \"item\")\n",
    "    .load(\"/Volumes/vstone-catalog/vstone_schema/chunked_data/chunk4/menu_items.xml\")\n",
    ")\n",
    "\n",
    "basic_profile(\n",
    "    df=menu_items_df,\n",
    "    table_name=\"menu_items\",\n",
    "    pk_cols=[\"item_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9576b663-a06c-4a90-aa38-a0ec876bad31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "menu_items_df.describe().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa7c9b26-9480-4d79-9816-c0d3c87a43eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "payment_methods_df = (\n",
    "    spark.read\n",
    "    .format(\"xml\")\n",
    "    .option(\"rowTag\", \"item\")\n",
    "    .load(\"/Volumes/vstone-catalog/vstone_schema/chunked_data/chunk4/payment_methods.xml\")\n",
    ")\n",
    "basic_profile(\n",
    "    df=payment_methods_df,\n",
    "    table_name=\"payment_methods\",\n",
    "    pk_cols=[\"method_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b7488d4-53c8-4d28-a7fa-fe53909687f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "payment_methods_df.describe().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9df1cddf-8db2-4a02-8bee-f7dd119273d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stores_df = (\n",
    "    spark.read\n",
    "    .format(\"xml\")\n",
    "    .option(\"rowTag\", \"item\")\n",
    "    .load(\"/Volumes/vstone-catalog/vstone_schema/chunked_data/chunk4/stores.xml\")\n",
    ")\n",
    "basic_profile(\n",
    "    df=stores_df,\n",
    "    table_name=\"stores\",\n",
    "    pk_cols=[\"store_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd10feb8-3b73-4399-9aa5-3293623303d9",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767776817953}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stores_df.describe().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1de99195-7e70-4ee8-a69b-2bc8bbab584b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data profile.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
