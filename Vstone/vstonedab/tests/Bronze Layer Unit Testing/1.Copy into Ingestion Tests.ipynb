{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86f950d7-6320-4e2a-96b9-6096a0051bb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook establishes a Validation and Testing Suite for the COPY INTO ingestion process (Chunk 1). \n",
    "It ensures that the dynamic table identifiers and target schemas meet enterprise standards before executing large-scale data movements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48870cd5-dc19-4950-9d85-864e6a1ce2ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Why COPY INTO for Transaction Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49d246d5-9572-41ae-bb0e-6a989e874c5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We have selected the COPY INTO command for the Transactions dataset (Chunk 1) because it is the industry-standard choice for high-volume, batch-oriented CSV ingestion in Databricks.\n",
    "\n",
    " **Business & Industry Advantages\n",
    "Idempotency & Cost Savings:**\n",
    "\n",
    "- COPY INTO automatically keeps track of which files have already been loaded. From a business perspective, this prevents duplicate data and saves significant compute costs by avoiding the reprocessing of older files.\n",
    "\n",
    "**Reliability in Batch Ingestion:** \n",
    "\n",
    "- In the retail and finance industries, transaction data is often delivered in large monthly or daily batches. COPY INTO is a robust, low-complexity tool that provides a simplified \"SQL-first\" experience for managing these loads.\n",
    "\n",
    "**Built-in Auditability:**\n",
    "\n",
    "- It supports the inclusion of file metadata (like _metadata.file_path) directly during the load, which is critical for meeting data governance and lineage requirements in regulated industries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38ad2171-c5c6-4f11-82fe-3f40775a6351",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Core Ingestion Logic\n",
    "This section isolates the logic used to build table paths and SQL commands, making the ingestion pipeline modular.\n",
    "\n",
    "####Logic: \n",
    "\n",
    "The functions handle the dynamic construction of Unity Catalog identifiers and the generation of the COPY INTO SQL statement.\n",
    "\n",
    "####Why this code: \n",
    " \n",
    " Separating the string-building logic from the execution allows for Unit Testing. We can verify the SQL is correct before running it against a live cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6c358d9-3e1b-46ad-898c-d05cc7377281",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_table_identifier(catalog, schema, table=\"transactions_bronze\"):\n",
    "    \"\"\"\n",
    "    Logic: Dynamically constructs the 3-tier Unity Catalog table path.\n",
    "    Why: Ensures the pipeline can target different environments (Dev/Prod) without hardcoding.\n",
    "    \"\"\"\n",
    "    if not catalog or not schema:\n",
    "        return f\"test_temp_{table}\" # Fallback for local/CI testing\n",
    "    return f\"`{catalog}`.`{schema}`.`{table}`\"\n",
    "\n",
    "def get_ingestion_sql(table_name, source_path):\n",
    "    \"\"\"\n",
    "    Logic: Generates the specialized COPY INTO command.\n",
    "    Why: Centralizes the command structure for easier maintenance and schema evolution.\n",
    "    \"\"\"\n",
    "    return f\"COPY INTO {table_name} FROM '{source_path}' FILEFORMAT = CSV\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7b2c3e8-9de0-483a-9bd4-666b1dd6ae8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Unit and Integration Test Suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f690fbc4-aeb5-4815-a7ae-1c7cfcdad4be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This block validates the logic and the environment using the unittest framework and Spark testing utilities.\n",
    "\n",
    "####Logic:\n",
    "\n",
    "- **Path Generation Test:** Validates that the table identifier correctly follows the `catalog`.`schema`.`table` format.\n",
    "\n",
    "- **Schema Audit Test:** Verifies that the required audit columns (load_dt and source_file) are present in the target table schema.\n",
    "\n",
    "####Why this code:\n",
    "\n",
    "These tests serve as a \"Pre-Flight Check.\" By validating the schema and paths, we ensure the pipeline won't fail midway through a multi-million row transaction load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "815f2d21-3a16-44b0-b1bc-e21f76bfba31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "import io\n",
    "from unittest import TextTestRunner\n",
    "from pyspark.testing.utils import assertSchemaEqual\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "\n",
    "class IngestionTest(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        cls.spark = spark\n",
    "        cls.test_table = \"temp_test_ingest\"\n",
    "\n",
    "    def test_dynamic_path_generation(self):\n",
    "        \"\"\"Unit Test: Ensure table identifier logic is correct and escaped\"\"\"\n",
    "        actual = get_table_identifier(\"dev\", \"raw\", \"orders\")\n",
    "        self.assertEqual(actual, \"`dev`.`raw`.`orders`\")\n",
    "\n",
    "    def test_schema_audit_columns(self):\n",
    "        \"\"\"Integration Test: Validate existence of mandatory audit columns\"\"\"\n",
    "        # Setup: Create a local mock table with audit columns\n",
    "        self.spark.sql(f\"CREATE TABLE IF NOT EXISTS {self.test_table} (load_dt TIMESTAMP, source_file STRING)\")\n",
    "        \n",
    "        expected_schema = StructType([\n",
    "            StructField(\"load_dt\", TimestampType(), True),\n",
    "            StructField(\"source_file\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        actual_schema = self.spark.table(self.test_table).schema\n",
    "        # Logic: Confirm the actual table matches the required enterprise audit schema\n",
    "        assertSchemaEqual(actual_schema, expected_schema)\n",
    "\n",
    "# Load the tests into the execution suite\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(IngestionTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d3c9701-4fc2-4bff-b8a5-5c77ea24e6cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3.Reporting and Quality Verdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "113903b3-2b1a-414b-93eb-95310a52af88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The final section executes the tests and generates a formatted Ingestion Pipeline Quality Report.\n",
    "\n",
    "####Logic: \n",
    "\n",
    "It captures the test output into a string buffer to present a clean summary of successes and failures. \n",
    "\n",
    "####Why this code:\n",
    "\n",
    "This provides a clear \"Success\" signal to the data engineering team. If the report shows failures, the notebook includes instructions to use %debug to inspect the logic immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf53aa95-b257-4c84-8c9d-9f0010220da5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Capture report in a stream\n",
    "stream = io.StringIO()\n",
    "runner = TextTestRunner(stream=stream, verbosity=2)\n",
    "result = runner.run(suite)\n",
    "\n",
    "# 2. Format the Final Report\n",
    "print(\"●●● INGESTION PIPELINE QUALITY REPORT ●●●\")\n",
    "print(\"-\" * 45)\n",
    "print(stream.getvalue())\n",
    "print(\"-\" * 45)\n",
    "print(f\"TOTAL TESTS: {result.testsRun}\")\n",
    "print(f\"SUCCESSES: {result.testsRun - len(result.failures) - len(result.errors)}\")\n",
    "print(f\"FAILURES/ERRORS: {len(result.failures) + len(result.errors)}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "if not result.wasSuccessful():\n",
    "    print(\"\\n[INSTRUCTION] To debug the ERROR above, run '%debug' in the next cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27ec2feb-4399-4e4b-80e3-baeed16e3453",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Report Summary**\n",
    "- Total Tests Run: 2\n",
    "\n",
    "- Successes: 2\n",
    "\n",
    "- Status: ✅ OK"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1.Copy into Ingestion Tests",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
