{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "795f9a14-4e7f-49ea-8680-2d96147d0311",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook establishes a Validation and Testing Suite for the incremental ingestion of transaction items (Chunk 2). \n",
    "\n",
    "It utilizes the unittest framework to verify that the transformation logic used within Delta Live Tables (DLT) is accurate and that the source data volumes are correctly configured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbea8c76-d5a7-4f88-9ce3-6a6ed1737cc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Why Incremental Loading for Transaction Items?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7375b56a-e3ec-4a94-8e49-6adeb248ca2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We have specifically selected the Transaction Items dataset for the incremental loading method (Chunk 2) for the following reasons:\n",
    "\n",
    "- **High Data Volume:** Transaction items represent the largest dataset in the pipeline, containing millions of rows that describe individual product lines for every purchase.\n",
    " \n",
    "- **Efficiency:** Unlike a full refresh, incremental loading only processes new or changed data since the last ingestion, significantly reducing cluster compute costs and processing time.\n",
    "\n",
    "- **Data Freshness:** This method allows for frequent, low-latency updates, ensuring that downstream analytics reflect the latest sales data without the overhead of re-reading the entire historical dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9be9361d-f4ee-4383-a2c9-2bce11c66fd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1. Core Transformation Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a04d59dd-724b-4497-bce5-c775dd116fb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The business logic for preparing incremental data is isolated into a single function to ensure it can be validated independently of the DLT pipeline.\n",
    "\n",
    "####Logic: \n",
    "\n",
    "The transform_transaction_items function appends necessary audit metadata to each record. \n",
    "\n",
    "####Why this code: \n",
    "\n",
    "Isolating this logic allows us to verify that audit columns like load_dt and source are added correctly before the data is committed to the Bronze layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77778ea0-bdb6-49f3-9a06-c3471bce703e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "def transform_transaction_items(df):\n",
    "    \"\"\"\n",
    "    Core transformation logic for Chunk 2.\n",
    "    Adds load_dt and source metadata columns.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df.withColumn(\"load_dt\", current_timestamp())\n",
    "          .withColumn(\"source\", lit(\"chunk2_csv\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa69afa9-24c4-4561-980f-6e32c4aa9e01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Unit and Integration Test Suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de2c7972-92f0-4d4d-9cbd-fd62c6b3386e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This section defines the test cases required to ensure the stability of the incremental ingestion process.\n",
    "\n",
    "####Logic:\n",
    "\n",
    "Unit Test (test_transformation_logic): Uses mock data to verify that the transformation function adds the required columns and tags correctly.\n",
    "\n",
    "####Integration Test (test_integration_path_check): \n",
    "Directly checks the Unity Catalog Volume to ensure the source files are physically accessible by the compute cluster.\n",
    "\n",
    "####Why this code: \n",
    "\n",
    "These tests act as a \"gatekeeper\" for the DLT pipeline. By verifying logic and path accessibility upfront, we prevent pipeline failures that are often difficult to debug during live streaming runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "007b7fe2-0c54-40e6-8544-0d91fc254d39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "import io\n",
    "from unittest import TextTestRunner\n",
    "\n",
    "class DLTIncrementalTest(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        # Access the existing SparkSession\n",
    "        cls.spark = spark\n",
    "\n",
    "    def test_transformation_logic(self):\n",
    "        \"\"\"Unit Test: Verify metadata columns are added correctly\"\"\"\n",
    "        # Create mock input data\n",
    "        input_data = [(\"TXN123\", \"ITEM456\")]\n",
    "        input_df = self.spark.createDataFrame(input_data, [\"transaction_id\", \"item_id\"])\n",
    "        \n",
    "        # Apply transformation\n",
    "        output_df = transform_transaction_items(input_df)\n",
    "        \n",
    "        # Validate columns and content\n",
    "        self.assertIn(\"load_dt\", output_df.columns)\n",
    "        self.assertIn(\"source\", output_df.columns)\n",
    "        self.assertEqual(output_df.collect()[0][\"source\"], \"chunk2_csv\")\n",
    "\n",
    "    def test_integration_path_check(self):\n",
    "        \"\"\"Integration Test: Verify source Volume accessibility\"\"\"\n",
    "        source_path = \"/Volumes/vstone-catalog/vstone_schema/chunked_data/chunk2/transaction_items/\"\n",
    "        try:\n",
    "            dbutils.fs.ls(source_path)\n",
    "            path_exists = True\n",
    "        except:\n",
    "            path_exists = False\n",
    "        \n",
    "        print(f\"Integration Check - Path {source_path} accessible: {path_exists}\")\n",
    "        self.assertTrue(path_exists)\n",
    "\n",
    "# Initialize the suite\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(DLTIncrementalTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b239300e-4f66-445b-bfae-622e426877ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Reporting and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d2a0c4d-74ea-4795-90b4-37bc15df6894",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The final block executes the tests and generates a formatted DLT Ingestion Pipeline Test Report.\n",
    "\n",
    "####Logic:\n",
    "\n",
    "The script captures test results into a stream and prints a summary including the count of successes, failures, and errors. \n",
    " \n",
    "####Why this code: \n",
    "This report provides a clear \"Go/No-Go\" signal for the deployment of the incremental ingestion job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb9b8702-bf53-4958-a66c-3918ab3488bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Capture report output\n",
    "stream = io.StringIO()\n",
    "runner = TextTestRunner(stream=stream, verbosity=2)\n",
    "result = runner.run(suite)\n",
    "\n",
    "# Generate Final Report\n",
    "print(\"●●● DLT INGESTION PIPELINE TEST REPORT ●●●\")\n",
    "print(\"-\" * 45)\n",
    "print(stream.getvalue())\n",
    "print(\"-\" * 45)\n",
    "print(f\"TOTAL TESTS RUN: {result.testsRun}\")\n",
    "print(f\"SUCCESSES: {result.testsRun - len(result.failures) - len(result.errors)}\")\n",
    "print(f\"FAILURES: {len(result.failures)}\")\n",
    "print(f\"ERRORS: {len(result.errors)}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "if not result.wasSuccessful():\n",
    "    print(\"\\n[DEBUG] Failure detected. Use %debug to inspect the transformation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17f24c9d-62ec-4c12-adc7-641793d4c55a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Report Summary**\n",
    "- Total Tests Run: 2\n",
    "\n",
    "- Successes: 2\n",
    "\n",
    "- Failures: 0\n",
    "\n",
    "- Path Accessibility: Verified (True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2.Incremental Loading Ingestion Tests",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
