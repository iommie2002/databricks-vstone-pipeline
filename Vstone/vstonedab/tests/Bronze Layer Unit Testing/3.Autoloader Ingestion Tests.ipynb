{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89883805-1d39-4cdc-b597-897690076709",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "This notebook establishes a Unit Testing Framework for the Auto Loader ingestion pipeline (Chunk 3). It focuses on validating the core transformation logic and environment configuration independently of the streaming process, ensuring high code quality and reliable data ingestion into the Bronze layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a13a418-88c7-4c1d-a15d-078588179436",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1. Refactored Transformation Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43fd90ba-e744-48c3-8f8f-10fe875dcdfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "To enable effective testing, the ingestion logic is refactored into modular, pure functions.\n",
    "* transform_json_ingestion: This function encapsulates the core transformation logic.\n",
    "####Logic: \n",
    "It adds technical metadata, specifically a load timestamp (load_dt) and a source identifier (source), to the incoming stream.\n",
    "* get_table_name: This function handles environment-specific configuration.\n",
    "####Logic:\n",
    " It dynamically constructs the full Unity Catalog table identifier using provided catalog and schema names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fc7256b-e1f4-4b5b-968a-4334f8900351",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "import io\n",
    "from unittest import TextTestRunner\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96c55ed0-5067-4995-ae62-f7c355c48a05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 1. Refactored Logic for Transformation ---\n",
    "def transform_json_ingestion(df):\n",
    "    \"\"\"\n",
    "    Core transformation logic for Chunk 3.\n",
    "    Adds audit columns to the stream.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df.withColumn(\"load_dt\", current_timestamp())\n",
    "          .withColumn(\"source\", lit(\"dab_json_ingestion\"))\n",
    "    )\n",
    "\n",
    "def get_table_name(catalog, schema):\n",
    "    \"\"\"Logic for Environment Configuration\"\"\"\n",
    "    return f\"`{catalog}`.`{schema}`.`users_bronze`\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "929f0b16-780c-49c8-b9dc-bd4f95412312",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2.Unit Test Suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f055cb4f-593d-413b-a39e-f4d15cf3671e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "The test suite utilizes the unittest framework to verify that the refactored functions behave correctly under various scenarios.\n",
    "\n",
    "* **test_transformation_metadata:** Validates the addition of audit columns.\n",
    "\n",
    "* **Logic:** It creates a mock DataFrame using pyspark.sql.Row and asserts that the resulting DataFrame contains both the load_dt and source columns with correct values.\n",
    "\n",
    "* **test_environment_widget_logic:** Ensures correct table naming.\n",
    "\n",
    "* **Logic**: It asserts that the get_table_name function correctly formats the 3-tier namespace string required for Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a98e0d9-81ac-4cfe-ba06-e2a36f200e2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- 2. Test Suite (Only Unit Tests) ---\n",
    "class AutoloaderIngestionTest(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        # Access the active Spark session (Spark Connect compatible)\n",
    "        cls.spark = spark\n",
    "\n",
    "    def test_transformation_metadata(self):\n",
    "        \"\"\"Unit Test: Verify audit columns (load_dt, source) are added\"\"\"\n",
    "        # Create data using Rows to avoid JVM/sparkContext dependencies\n",
    "        input_data = [Row(user_id=1, name=\"John\")]\n",
    "        input_df = self.spark.createDataFrame(input_data)\n",
    "        \n",
    "        output_df = transform_json_ingestion(input_df)\n",
    "        \n",
    "        # Verify columns exist\n",
    "        self.assertIn(\"load_dt\", output_df.columns)\n",
    "        self.assertIn(\"source\", output_df.columns)\n",
    "        \n",
    "        # Verify constant value matches logic\n",
    "        actual_source = output_df.select(\"source\").distinct().collect()[0][0]\n",
    "        self.assertEqual(actual_source, \"dab_json_ingestion\")\n",
    "\n",
    "    def test_environment_widget_logic(self):\n",
    "        \"\"\"Unit Test: Ensure table identifier construction is correct\"\"\"\n",
    "        actual_name = get_table_name(\"prod_catalog\", \"bronze_db\")\n",
    "        self.assertEqual(actual_name, \"`prod_catalog`.`bronze_db`.`users_bronze`\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec3bd083-eb1a-4bcb-a715-5b7d0b74fcc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Quality Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "131e14b1-6783-48a8-93a0-3b04eba0cd9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The final section executes the test suite and generates a structured Pipeline Quality Report.\n",
    "\n",
    "- **Execution:** The TextTestRunner runs the suite and captures detailed logs in an io.StringIO stream.\n",
    "\n",
    "- **Summary:** The report displays the total number of tests, successes, and any failures or errors.\n",
    "\n",
    "- **Verdict:** A final status message indicates whether all logic and unit tests are verified or if issues were detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18a487ba-0705-4ad5-b4c6-34690e79b659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 3. Run and Generate Report ---\n",
    "# Load the tests into the suite\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(AutoloaderIngestionTest)\n",
    "\n",
    "# Create a stream to capture the results\n",
    "stream = io.StringIO()\n",
    "runner = TextTestRunner(stream=stream, verbosity=2)\n",
    "\n",
    "# Run the suite\n",
    "result = runner.run(suite)\n",
    "\n",
    "# Print the formatted report\n",
    "print(\"●●● AUTOLOADER PIPELINE QUALITY REPORT ●●●\")\n",
    "print(\"-\" * 45)\n",
    "print(stream.getvalue())\n",
    "print(\"-\" * 45)\n",
    "print(f\"TOTAL TESTS: {result.testsRun}\")\n",
    "print(f\"SUCCESSES: {result.testsRun - len(result.failures) - len(result.errors)}\")\n",
    "print(f\"FAILURES/ERRORS: {len(result.failures) + len(result.errors)}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "if result.wasSuccessful():\n",
    "    print(\"STATUS: PASSED - All logic and unit tests are verified.\")\n",
    "else:\n",
    "    print(\"STATUS: FAILED - Logic validation issues detected.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3.Autoloader Ingestion Tests",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
