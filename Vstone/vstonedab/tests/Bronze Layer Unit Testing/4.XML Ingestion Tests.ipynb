{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bffd3fd-ec6c-4d5f-b876-99c17a6690ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "This notebook establishes a Quality Assurance and Testing Suite specifically for the XML ingestion components of the data pipeline. It ensures that the modular ingestion logic correctly handles audit metadata and that the Databricks environment properly communicates with job parameters via widgets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bb08ffa-5ba5-4c38-bd06-bd126a875bef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1.Core Transformation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e25f614-23de-465d-9de0-25cfa28e1b44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The primary business logic for XML processing is encapsulated in a dedicated function to allow for isolated testing.\n",
    "\n",
    "####Logic: \n",
    "\n",
    "The function transform_xml_data takes a raw DataFrame and appends technical audit columns: a current timestamp and a string literal identifying the data source. \n",
    "\n",
    "####Why this code:\n",
    "\n",
    " Isolating this logic from the database write operations allows us to run unit tests in-memory, ensuring the data is correctly \"tagged\" before it ever hits the Bronze layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f53f684e-75b2-496d-a69b-4f5a28868a24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "def transform_xml_data(df, source_tag):\n",
    "    \"\"\"\n",
    "    Logic: Core transformation that adds audit columns.\n",
    "    Used for unit testing without writing to Delta tables.\n",
    "    \"\"\"\n",
    "    # Append the load timestamp and the specific source identifier\n",
    "    return (df.withColumn(\"load_dt\", current_timestamp())\n",
    "            .withColumn(\"source\", lit(source_tag)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b49dd901-09bf-47ea-be63-c3a11667f949",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2.Unit and Integration Test Suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abf54664-66a1-4cd5-95f3-27fdecac341d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This section utilizes the unittest framework to validate both the data transformation logic and the Databricks environment integration.\n",
    "\n",
    "####Logic:\n",
    "\n",
    "* Transformation Test: Creates a mock DataFrame and asserts that the load_dt and source columns are present and contain the expected values.\n",
    "\n",
    "* Integration Test: Mocks the creation and retrieval of Databricks widgets to ensure the pipeline can receive dynamic parameters from external Job tasks.\n",
    "\n",
    "####Why this code: \n",
    "\n",
    "Automated testing is critical for maintainability. These tests verify that any changes to the global ingestion script won't break the specific requirements for XML sources like vouchers or stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25d53333-6d6b-446e-9b4d-8278ba6c7702",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "import io\n",
    "from unittest import TextTestRunner\n",
    "from pyspark.testing.utils import assertDataFrameEqual, assertSchemaEqual\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "class XMLIngestionTest(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        # Attach the existing SparkSession to the test class\n",
    "        cls.spark = spark\n",
    "\n",
    "    def test_xml_transformation_columns(self):\n",
    "        \"\"\"Unit Test: Verify audit columns are added to the XML structure\"\"\"\n",
    "        # Create mock data mimicking an XML item\n",
    "        data = [(\"101\", \"VoucherA\")]\n",
    "        input_df = self.spark.createDataFrame(data, [\"voucher_id\", \"voucher_code\"])\n",
    "        \n",
    "        # Apply the transformation function\n",
    "        output_df = transform_xml_data(input_df, \"vstone_vouchers\")\n",
    "        \n",
    "       # Assertions to ensure audit columns exist and are correct\n",
    "        self.assertIn(\"load_dt\", output_df.columns)\n",
    "        self.assertIn(\"source\", output_df.columns)\n",
    "        actual_source = output_df.select(\"source\").first()[0]\n",
    "        self.assertEqual(actual_source, \"vstone_vouchers\")\n",
    "\n",
    "    def test_integration_widget_params(self):\n",
    "        \"\"\"Integration Test: Validate widget retrieval logic for job parameters\"\"\"\n",
    "        # Simulate setting a widget in Databricks\n",
    "        dbutils.widgets.text(\"test_tag\", \"mock_xml\")\n",
    "        retrieved_tag = dbutils.widgets.get(\"test_tag\")\n",
    "        self.assertEqual(retrieved_tag, \"mock_xml\")\n",
    "\n",
    "# Load the test cases into a suite for execution\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(XMLIngestionTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a0cd2aa-5e88-40fc-8fb9-75160b6f31d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Execution and Quality Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cf9eac8-5021-4ee6-92f6-9fc5bbcf14ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The final block runs the suite and generates a structured Quality Report to provide clear visibility into the health of the ingestion logic.\n",
    "\n",
    "####Logic: \n",
    "\n",
    "\n",
    "The script executes the tests through a TextTestRunner and captures the results in a string buffer. It then parses these results to display a final summary of passes, failures, and errors. \n",
    "\n",
    "####Why this code: \n",
    "\n",
    "This reporting format is essential for automated workflows. If this notebook is run as part of a CI/CD pipeline, the \"Alert\" logic at the end will trigger a failure if any test does not pass, preventing faulty code from being deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cced74a0-8033-4c4a-bbe0-710a36e2bcc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Setup reporting stream to capture test results\n",
    "stream = io.StringIO()\n",
    "runner = TextTestRunner(stream=stream, verbosity=2)\n",
    "\n",
    "# 2. Execute the tests\n",
    "result = runner.run(suite)\n",
    "\n",
    "# 3. Print the Final Formatted Report\n",
    "print(\"●●● XML INGESTION QUALITY REPORT ●●●\")\n",
    "print(\"-\" * 45)\n",
    "print(stream.getvalue())\n",
    "print(\"-\" * 45)\n",
    "print(f\"TOTAL TESTS RUN: {result.testsRun}\")\n",
    "print(f\"PASSED: {result.testsRun - len(result.failures) - len(result.errors)}\")\n",
    "print(f\"FAILED: {len(result.failures)}\")\n",
    "print(f\"ERRORS: {len(result.errors)}\")\n",
    "print(\"-\" * 45)\n",
    "# Final verdict logic for automated pipelines\n",
    "if not result.wasSuccessful():\n",
    "    print(\"\\n[ALERT] Logic validation failed. Run '%debug' to investigate.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4.XML Ingestion Tests",
   "widgets": {
    "test_tag": {
     "currentValue": "mock_xml",
     "nuid": "80c978cc-5a92-4cc8-86da-11fdc0583d09",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "mock_xml",
      "label": null,
      "name": "test_tag",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "mock_xml",
      "label": null,
      "name": "test_tag",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
