{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5af39a5a-db70-4d01-89be-40345fba80b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def transform_users_silver(df_bronze):\n",
    "    \"\"\"\n",
    "    Modular transformation function.\n",
    "    Includes NEW normalization logic to ensure data quality.\n",
    "    \"\"\"\n",
    "    # 1. Standardize column headers to snake_case\n",
    "    standardized_cols = [col.lower().replace(\" \", \"_\") for col in df_bronze.columns]\n",
    "    df_standardized = df_bronze.toDF(*standardized_cols)\n",
    "\n",
    "    # 2. Quality Gates: user_id must exist and email must be valid\n",
    "    id_valid = F.col(\"user_id\").isNotNull()\n",
    "    email_valid = F.col(\"email\").contains(\"@\")\n",
    "    valid_mask = id_valid & email_valid\n",
    "\n",
    "    # 3. Filter clean data\n",
    "    df_clean = df_standardized.filter(valid_mask)\n",
    "\n",
    "    # 4. NORMALIZATION (Added these steps to ensure a successful test)\n",
    "    # We trim whitespace and force lowercase for consistent lookups\n",
    "    df_normalized = df_clean.withColumn(\"email\", F.lower(F.trim(F.col(\"email\")))) \\\n",
    "                            .withColumn(\"load_dt\", F.to_timestamp(F.col(\"load_dt\")))\n",
    "\n",
    "    # 5. Deduplication: Keep only the latest record\n",
    "    window_spec = Window.partitionBy(\"user_id\").orderBy(F.col(\"load_dt\").desc())\n",
    "\n",
    "    df_silver_final = df_normalized.withColumn(\"row_rank\", F.row_number().over(window_spec)) \\\n",
    "        .filter(\"row_rank == 1\") \\\n",
    "        .drop(\"row_rank\") \\\n",
    "        .select(\"user_id\", \"user_name\", \"email\", \"load_dt\")\n",
    "\n",
    "    return df_silver_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d921434-321a-4f20-9880-0206668e2c23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "import pandas as pd\n",
    "import io\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.testing import assertDataFrameEqual\n",
    "\n",
    "class TestUsersSilver(unittest.TestCase):\n",
    "    def test_email_normalization_and_deduplication(self):\n",
    "        # MOCK INPUT: Messy casing and extra spaces\n",
    "        input_data = [(201, \"Alice\", \" ALICE@work.com \", \"2026-01-02 10:00:00\")]\n",
    "        input_schema = StructType([\n",
    "            StructField(\"user_id\", LongType(), True),\n",
    "            StructField(\"user_name\", StringType(), True),\n",
    "            StructField(\"email\", StringType(), True),\n",
    "            StructField(\"load_dt\", StringType(), True)\n",
    "        ])\n",
    "        df_input = spark.createDataFrame(input_data, input_schema)\n",
    "\n",
    "        # EXPECTED OUTPUT: Cleaned and localized\n",
    "        expected_ts = pd.Timestamp(\"2026-01-02 10:00:00\").tz_localize('UTC')\n",
    "        expected_data = [(201, \"Alice\", \"alice@work.com\", expected_ts)]\n",
    "        \n",
    "        expected_schema = StructType([\n",
    "            StructField(\"user_id\", LongType(), True),\n",
    "            StructField(\"user_name\", StringType(), True),\n",
    "            StructField(\"email\", StringType(), True),\n",
    "            StructField(\"load_dt\", TimestampType(), True)\n",
    "        ])\n",
    "        df_expected = spark.createDataFrame(expected_data, expected_schema)\n",
    "\n",
    "        # ACT: Run the function\n",
    "        df_actual = transform_users_silver(df_input)\n",
    "\n",
    "        # ASSERT: This would fail if normalization wasn't implemented\n",
    "        assertDataFrameEqual(df_actual, df_expected)\n",
    "\n",
    "# Run the test\n",
    "runner = unittest.TextTestRunner(verbosity=2)\n",
    "runner.run(unittest.TestLoader().loadTestsFromTestCase(TestUsersSilver))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Users Test",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
