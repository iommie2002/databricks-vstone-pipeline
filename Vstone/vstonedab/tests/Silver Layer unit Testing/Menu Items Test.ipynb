{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11cded31-a3cf-4eb9-bc2d-b7ca96e10c03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import io\n",
    "import unittest\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.testing import assertDataFrameEqual, assertSchemaEqual\n",
    "\n",
    "def transform_menuitems_silver(df_bronze):\n",
    "    \"\"\"\n",
    "    Industry Standard Transformation Function.\n",
    "    Input: Bronze DataFrame (Raw)\n",
    "    Output: Silver DataFrame (Cleaned, Deduped, Validated)\n",
    "    \"\"\"\n",
    "    # 1. Standardization\n",
    "    standardized_cols = [col.lower().replace(\" \", \"_\") for col in df_bronze.columns]\n",
    "    df_standardized = df_bronze.toDF(*standardized_cols)\n",
    "    \n",
    "    # 2. Validation Mask\n",
    "    valid_mask = (F.col(\"item_id\").isNotNull()) & (F.col(\"price\") > 0)\n",
    "    \n",
    "    # 3. Clean, Cast, and Dedupe\n",
    "    window_spec = Window.partitionBy(\"item_id\").orderBy(F.col(\"load_dt\").desc())\n",
    "    \n",
    "    df_silver_final = df_standardized.filter(valid_mask) \\\n",
    "        .withColumn(\"row_rank\", F.row_number().over(window_spec)) \\\n",
    "        .filter(\"row_rank == 1\") \\\n",
    "        .drop(\"row_rank\") \\\n",
    "        .withColumn(\"price\", F.round(F.col(\"price\").cast(\"double\"), 2)) \\\n",
    "        .withColumn(\"load_dt\", F.to_timestamp(F.col(\"load_dt\"))) \\\n",
    "        .select(\"category\", \"is_seasonal\", \"item_id\", \"item_name\", \"price\", \"load_dt\", \"source\")\n",
    "        \n",
    "    return df_silver_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d1d9ca4-565e-4094-b907-170ae907e0c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import io\n",
    "import unittest\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.testing import assertDataFrameEqual, assertSchemaEqual\n",
    "\n",
    "def transform_menuitems_silver(df_bronze):\n",
    "    \"\"\"\n",
    "    Industry Standard Transformation Function.\n",
    "    Input: Bronze DataFrame (Raw)\n",
    "    Output: Silver DataFrame (Cleaned, Deduped, Validated)\n",
    "    \"\"\"\n",
    "    # 1. Standardization\n",
    "    standardized_cols = [col.lower().replace(\" \", \"_\") for col in df_bronze.columns]\n",
    "    df_standardized = df_bronze.toDF(*standardized_cols)\n",
    "    \n",
    "    # 2. Validation Mask\n",
    "    valid_mask = (F.col(\"item_id\").isNotNull()) & (F.col(\"price\") > 0)\n",
    "    \n",
    "    # 3. Clean, Cast, and Dedupe\n",
    "    window_spec = Window.partitionBy(\"item_id\").orderBy(F.col(\"load_dt\").desc())\n",
    "    \n",
    "    df_silver_final = df_standardized.filter(valid_mask) \\\n",
    "        .withColumn(\"row_rank\", F.row_number().over(window_spec)) \\\n",
    "        .filter(\"row_rank == 1\") \\\n",
    "        .drop(\"row_rank\") \\\n",
    "        .withColumn(\"price\", F.round(F.col(\"price\").cast(\"double\"), 2)) \\\n",
    "        .withColumn(\"load_dt\", F.to_timestamp(F.col(\"load_dt\"))) \\\n",
    "        .select(\"category\", \"is_seasonal\", \"item_id\", \"item_name\", \"price\", \"load_dt\", \"source\")\n",
    "        \n",
    "    return df_silver_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb40592c-3c8e-4566-b830-ff45391538fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class TestMenuitemsSilver(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        \"\"\"Setup input schema for mock data\"\"\"\n",
    "        self.schema = StructType([\n",
    "            StructField(\"category\", StringType(), True),\n",
    "            StructField(\"is_seasonal\", BooleanType(), True),\n",
    "            StructField(\"item_id\", LongType(), True),\n",
    "            StructField(\"item_name\", StringType(), True),\n",
    "            StructField(\"price\", DoubleType(), True),\n",
    "            StructField(\"load_dt\", StringType(), True),\n",
    "            StructField(\"source\", StringType(), True)\n",
    "        ])\n",
    "\n",
    "    def test_deduplication_and_validation(self):\n",
    "        # 1. CREATE MOCK BRONZE DATA\n",
    "        # Row 1: Valid\n",
    "        # Row 2: Duplicate (older, should be dropped)\n",
    "        # Row 3: Invalid price (should be filtered)\n",
    "        # Row 4: Null ID (should be filtered)\n",
    "        data = [\n",
    "            (\"Pizza\", False, 101, \"Margherita\", 12.50, \"2024-01-02 10:00:00\", \"POS\"),\n",
    "            (\"Pizza\", False, 101, \"Margherita\", 12.00, \"2024-01-01 10:00:00\", \"POS\"), \n",
    "            (\"Sides\", True, 102, \"Fries\", -5.00, \"2024-01-02 10:00:00\", \"Web\"),\n",
    "            (\"Sides\", True, None, \"Invalid\", 5.00, \"2024-01-02 10:00:00\", \"Web\")\n",
    "        ]\n",
    "        df_input = spark.createDataFrame(data, self.schema)\n",
    "\n",
    "        # 2. DEFINE EXPECTED OUTPUT\n",
    "        expected_schema = StructType([\n",
    "            StructField(\"category\", StringType(), True),\n",
    "            StructField(\"is_seasonal\", BooleanType(), True),\n",
    "            StructField(\"item_id\", LongType(), True),\n",
    "            StructField(\"item_name\", StringType(), True),\n",
    "            StructField(\"price\", DoubleType(), True),\n",
    "            StructField(\"load_dt\", TimestampType(), True),\n",
    "            StructField(\"source\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        # FIX: Explicitly localize the timestamp to UTC to solve the TypeError\n",
    "        expected_ts = pd.Timestamp(\"2024-01-02 10:00:00\").tz_localize('UTC')\n",
    "\n",
    "        expected_data = [\n",
    "            (\"Pizza\", False, 101, \"Margherita\", 12.50, expected_ts, \"POS\")\n",
    "        ]\n",
    "        df_expected = spark.createDataFrame(expected_data, expected_schema)\n",
    "\n",
    "        # 3. EXECUTE TRANSFORMATION\n",
    "        df_actual = transform_menuitems_silver(df_input)\n",
    "\n",
    "        # 4. ASSERTIONS\n",
    "        # Test Schema: Checks if load_dt was successfully converted to TimestampType\n",
    "        assertSchemaEqual(df_actual.schema, expected_schema)\n",
    "        \n",
    "        # Test Data: Checks if duplicates and nulls were correctly removed\n",
    "        assertDataFrameEqual(df_actual, df_expected)\n",
    "\n",
    "# --- EXECUTION AND REPORT GENERATION ---\n",
    "stream = io.StringIO()\n",
    "runner = unittest.TextTestRunner(stream=stream, verbosity=2)\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestMenuitemsSilver)\n",
    "result = runner.run(suite)\n",
    "\n",
    "print(f\"\"\"\n",
    "=========================================\n",
    "UNIT TEST REPORT - {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "=========================================\n",
    "Tests Run: {result.testsRun}\n",
    "Errors: {len(result.errors)}\n",
    "Failures: {len(result.failures)}\n",
    "Outcome: {'SUCCESS' if result.wasSuccessful() else 'FAILED'}\n",
    "-----------------------------------------\n",
    "Details:\n",
    "{stream.getvalue()}\n",
    "=========================================\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Menu Items Test",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
