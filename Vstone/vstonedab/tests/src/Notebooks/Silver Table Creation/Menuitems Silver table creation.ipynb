{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c64f5fe-55cd-417f-96fe-317481d10d67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook demonstrates a standard Medallion Architecture pattern, specifically focusing on the transition from the Bronze layer (raw data) to the Silver layer (cleaned, validated, and deduplicated data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66a5dafe-badd-4319-9824-026d0b7554be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Notebook Purpose:** This notebook implements the process to refine raw menu item data into a high-quality \"Silver\" table. It implements data quality checks, schema enforcement, and deduplication to ensure the data is \"Ready for Analytics.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68b455f8-a8f3-4e63-a4fb-6fa4f93f821b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1.Menuitems_silver table Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c38f8c8f-7488-40d8-85e0-da26327e071a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1. Environment Setup & Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cac0145-7528-4428-9975-c6747cde4375",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "define the naming conventions for our Delta tables and schemas using Unity Catalog naming standards (catalog.schema.table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e83bbd04-c028-43c0-8ab9-92d8143e370e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StringType, StructType, StructField, LongType, DoubleType, BooleanType, TimestampType\n",
    "\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Using backticks for catalog names containing hyphens to comply with Spark SQL syntax\n",
    "CATALOG = \"`vstone-catalog`\"\n",
    "SILVER_SCHEMA = \"silver_schema\"\n",
    "BRONZE_TABLE = f\"{CATALOG}.bronze_schema.bronze_menuitems\"\n",
    "SILVER_TABLE = f\"{CATALOG}.{SILVER_SCHEMA}.silver_menuitems\"\n",
    "QUARANTINE_TABLE = f\"{CATALOG}.{SILVER_SCHEMA}.quarantine_menuitems\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc269932-86c7-4f18-88a1-87847249c429",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Bootstrap the environment by ensuring the destination schema exists\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SILVER_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0bb95c1-e010-4652-95a2-d221562f6261",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Table Definition & Schema Enforcement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ded5497-ba91-4f48-940f-040a192c782d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Explicitly defining the Silver table schema ensures data integrity. We use the Delta Lake format to support ACID transactions and time travel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dc6b632-2170-4541-837f-a8c51cfa26b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- 2. EXPLICIT TABLE CREATION ---\n",
    "# Ensure the table is created with backticks in the identifier\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {SILVER_TABLE} (\n",
    "  category STRING,\n",
    "  is_seasonal BOOLEAN,\n",
    "  item_id BIGINT,\n",
    "  item_name STRING,\n",
    "  price DOUBLE,\n",
    "  load_dt TIMESTAMP,\n",
    "  source STRING\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Schema and Table {SILVER_TABLE} verified successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ab8bd32-7cdd-45b4-a6a2-326bdb21ab1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Data Ingestion & Header Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c93bc4ff-386a-459d-82c1-485910cba7a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Raw data often comes with inconsistent naming (spaces, special characters, mixed case). This step standardizes headers to snake_case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bb880af-7633-4353-98bf-8521069ef368",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- PANDAS UDF FOR HEADER STANDARDIZATION ---\n",
    "# Logic: Convert to lower, replace non-alphanumeric with underscores, strip trailing underscores\n",
    "@pandas_udf(StringType())\n",
    "def standardize_header_udf(col_name: pd.Series) -> pd.Series:\n",
    "    return col_name.str.lower().str.replace(r'[^a-zA-Z0-9]', '_', regex=True).str.strip('_')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f67f144-9b55-40f1-938d-b63c0920bd39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load raw data from Bronze\n",
    "try:\n",
    "    df_bronze = spark.read.table(BRONZE_TABLE)\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Bronze table not found: {e}\")\n",
    "\n",
    "# Apply standardization: industry standard is snake_case for database columns\n",
    "standardized_cols = [col.lower().replace(\" \", \"_\") for col in df_bronze.columns]\n",
    "df_standardized = df_bronze.toDF(*standardized_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59e784ee-5a52-4cff-a748-03ce64c4341e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##4. Data Quality & Quarantine Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c73c8ec4-8311-4d64-b4e5-041050ac39ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In production, we never \"drop\" bad data. Instead, we Quarantine it. This allows data engineers to investigate the source of error without stopping the entire pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59429136-65a9-4a0d-9ae8-92585b954d11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- QUARANTINE LOGIC (Malformed Records) ---\n",
    "# Validation Rules: \n",
    "# 1. Primary Key (item_id) must exist.\n",
    "# 2. Business Logic: Price must be a positive value.\n",
    "valid_mask = (F.col(\"item_id\").isNotNull()) & (F.col(\"price\") > 0)\n",
    "\n",
    "# Filter out failed records and tag them with a reason for easier debugging\n",
    "df_quarantine = df_standardized.filter(~valid_mask) \\\n",
    "    .withColumn(\"quarantine_reason\", \n",
    "        F.when(F.col(\"item_id\").isNull(), \"MISSING_ID\")\n",
    "         .otherwise(\"INVALID_PRICE\")) \\\n",
    "    .withColumn(\"quarantined_at\", F.current_timestamp())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c58fab88-e3ce-44e4-9299-d375fc67cfd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##5. Deduplication & Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccd3f7bb-cfcd-4739-be3f-76c742963818",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The Silver layer must represent the \"latest version of truth.\" We use Window functions to handle duplicate records for the same item_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "963349e9-678b-4516-9b85-dca68a48f148",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- CLEAN & DEDUPE ---\n",
    "# Partition by ID and order by timestamp to find the most recent record\n",
    "window_spec = Window.partitionBy(\"item_id\").orderBy(F.col(\"load_dt\").desc())\n",
    "\n",
    "df_silver_final = df_standardized.filter(valid_mask) \\\n",
    "    .withColumn(\"row_rank\", F.row_number().over(window_spec)) \\\n",
    "    .filter(\"row_rank == 1\") \\\n",
    "    .drop(\"row_rank\") \\\n",
    "    .withColumn(\"price\", F.round(F.col(\"price\").cast(\"double\"), 2)) \\\n",
    "    .withColumn(\"load_dt\", F.to_timestamp(F.col(\"load_dt\"))) \\\n",
    "    .select(\"category\", \"is_seasonal\", \"item_id\", \"item_name\", \"price\", \"load_dt\", \"source\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64f2d072-41e0-431e-b5a7-56ccac6d71be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6. Atomic Writes & Expectations\n",
    "Finally, we commit the data to the Delta tables and apply hard constraints to prevent future \"garbage\" data from entering via other processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f012c16f-77b0-457d-bf81-54e52393792b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- ATOMIC WRITES ---\n",
    "# Append failed records to Quarantine for audit trails)\n",
    "df_quarantine.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(QUARANTINE_TABLE)\n",
    "# Overwrite Silver table with the new \"gold standard\" clean set\n",
    "df_silver_final.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(SILVER_TABLE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c46d014-5131-4102-9971-05765ddcf116",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- APPLY CONSTRAINTS ---\n",
    "# Delta Constraints: These act as a firewall at the storage level\n",
    "\n",
    "spark.sql(f\"ALTER TABLE {SILVER_TABLE} CHANGE COLUMN item_id SET NOT NULL\")\n",
    "try:\n",
    "    spark.sql(f\"ALTER TABLE {SILVER_TABLE} ADD CONSTRAINT positive_price CHECK (price > 0)\")\n",
    "except:\n",
    "    pass # Constraint already exists\n",
    "\n",
    "print(f\"Successfully processed and updated: {SILVER_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "335612ab-2fa5-4c22-8f48-97701299585e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Industry Context & Logic\n",
    "\n",
    "**1. Why the Medallion Architecture?**\n",
    "\n",
    "In modern data engineering (Databricks/Spark), we use the Medallion Architecture:\n",
    "\n",
    "- Bronze: Raw, unvalidated data.\n",
    "\n",
    "- Silver (This Notebook): Cleaned, filtered, and deduplicated. It is the \"Source of Truth\" for Data Scientists.\n",
    "\n",
    "- Gold: Aggregated data for Business Intelligence (BI) dashboards.\n",
    "\n",
    "**2. Why Quarantine instead of Deleting?**\n",
    "\n",
    "If you simply delete records where price < 0, you lose visibility into upstream bugs. By writing to a quarantine_menuitems table, you provide a dashboard for data quality where engineers can see: \"Yesterday, 5% of our items had no IDs; we need to check the POS system source.\n",
    "\n",
    "**3. Why Use Delta Constraints?**\n",
    "Standard Spark dataframes are \"schema-on-read.\" \n",
    "By using ALTER TABLE ... ADD CONSTRAINT, we turn the Delta table into a \"schema-on-write\" system.\n",
    "\n",
    "This mimics traditional SQL Server/Oracle behavior, ensuring that no future notebook can accidentally write a negative price into our clean Silver table.\n",
    "\n",
    "**4. Deduplication Logic**\n",
    "\n",
    "The use of Window.partitionBy(\"item_id\").orderBy(F.col(\"load_dt\").desc()) is the industry-standard way to handle Late Arriving Data or duplicates.\n",
    "\n",
    "If a record for \"Coffee\" is sent twice, this logic ensures we only keep the one with the most recent load_dt."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Menuitems Silver table creation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
