{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04010716-a34a-4054-8649-efc7faeec033",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook follows the **Medallion Architecture** to refine raw transaction data from the **Bronze** layer into a validated, high-integrity **Silver** table. It focuses on header standardization, transaction-level validation, and deduplication to ensure the data is reliable for financial auditing and Gold-layer aggregations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b572b17-bf92-43b5-bc7a-5be02c6aab19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Silver Layer: Transactions Transformation\n",
    "**Notebook Objective:**\n",
    "\n",
    " This notebook automates the transition of header-level transaction data into the Silver layer. It enforces data quality gates, redirects malformed records to a quarantine table for auditing, and ensures a \"Single Version of Truth\" through deduplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa72b558-2d26-48e2-824c-c99b4df78059",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1. Initial Data Profiling (Bronze Layer)\n",
    "\n",
    "Before processing, we use SQL to check the health of the raw data, specifically looking for null values in critical financial and identification columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fbbf045-7153-482e-9739-747d72a6f51f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 1. Check for Nulls in critical business columns\n",
    "SELECT * FROM `vstone-catalog`.bronze_schema.transactions_bronze \n",
    "WHERE round(original_amount - discount_applied, 2) != round(final_amount, 2);\n",
    "\n",
    "-- 2. Validate financial logic: Total amount must be positive\n",
    "SELECT count(*) FROM `vstone-catalog`.bronze_schema.transactions_bronze \n",
    "WHERE transaction_id IS NULL OR store_id IS NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4dde4d68-3995-4bec-8e20-37a9daef40ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Configuration & Environment Setup\n",
    "\n",
    "We establish the naming conventions for our Delta tables using Unity Catalog standards (catalog.schema.table) and initialize the destination schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6d4b2bf-d7c5-43a2-9599-90c1b139bdc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType, DecimalType\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "# Hyphenated catalogs require backticks for Spark SQL compatibility\n",
    "CATALOG = \"`vstone-catalog`\"\n",
    "SILVER_SCHEMA = \"silver_schema\"\n",
    "BRONZE_TABLE = f\"{CATALOG}.bronze_schema.transactions_bronze\"\n",
    "SILVER_TABLE = f\"{CATALOG}.{SILVER_SCHEMA}.silver_transactions\"\n",
    "QUARANTINE_TABLE = f\"{CATALOG}.{SILVER_SCHEMA}.quarantine_transactions\"\n",
    "\n",
    "##Bootstrap: Ensure the Silver schema exists before writing\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SILVER_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b34d8a71-14a9-475d-8905-9eeb700f7d3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Data Ingestion & Header Normalization\n",
    "Raw system headers are often inconsistent. We standardize them to snake_case to maintain a uniform schema across the Lakehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d5ba460-2ff5-436e-8967-b9e200e9b0ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 2. LOAD & STANDARDIZE ---\n",
    "df_bronze = spark.read.table(BRONZE_TABLE)\n",
    "\n",
    "# Convert all column names to lowercase and replace spaces with underscores\n",
    "standardized_cols = [col.lower().replace(\" \", \"_\").strip() for col in df_bronze.columns]\n",
    "df_standardized = df_bronze.toDF(*standardized_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e7b9b41-4ad1-45e4-8d2c-1d6c657dace7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 3. DATA TYPING ---\n",
    "df_casted = df_standardized.select(\n",
    "    \"transaction_id\", \"store_id\", \"payment_method_id\", \"voucher_id\", \"user_id\",\n",
    "    F.col(\"original_amount\").cast(DecimalType(18, 2)).alias(\"original_amount\"),\n",
    "    F.col(\"discount_applied\").cast(DecimalType(18, 2)).alias(\"discount_applied\"),\n",
    "    F.col(\"final_amount\").cast(DecimalType(18, 2)).alias(\"final_amount\"),\n",
    "    F.to_timestamp(F.col(\"created_at\")).alias(\"created_at\"),\n",
    "    \"source_file\", \"load_dt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97f29951-7e33-4c82-b5ca-80f64c9c6396",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##4. Quality Gates & Quarantine Pattern\n",
    "To maintain high data integrity without crashing the pipeline, we use a Quarantine Pattern. Invalid transactions are tagged and diverted for review, while clean records move forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd614d86-f3a9-43f8-ba43-769063a29ef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 3. QUALITY GATES ---\n",
    "# Business Rules: \n",
    "# 1. Primary keys (transaction_id, store_id) must exist.\n",
    "# 2. Total amount must be a positive value.\n",
    "orig = F.coalesce(F.col(\"original_amount\"), F.lit(0))\n",
    "disc = F.coalesce(F.col(\"discount_applied\"), F.lit(0))\n",
    "final = F.coalesce(F.col(\"final_amount\"), F.lit(0))\n",
    "\n",
    "# UPDATED Business Rules: Adding user_id check\n",
    "math_valid = (orig - disc == final)\n",
    "id_valid = (F.col(\"transaction_id\").isNotNull()) & (F.col(\"store_id\").isNotNull())\n",
    "user_valid = (F.col(\"user_id\").isNotNull()) # <--- YOUR NEW RULE\n",
    "\n",
    "# Combined Mask\n",
    "valid_mask = math_valid & id_valid & user_valid\n",
    "\n",
    "# Isolate Malformed Records\n",
    "# Divert failed records to Quarantine with a reason tag\n",
    "df_quarantine = df_casted.filter(~valid_mask) \\\n",
    "    .withColumn(\"calculated_diff\", (orig - disc) - final) \\\n",
    "    .withColumn(\"quarantine_reason\", \n",
    "        F.when(~id_valid, \"MISSING_MANDATORY_ID\")\n",
    "         .when(~user_valid, \"NULL_USER_ID\") # <--- SPECIFIC REASON\n",
    "         .otherwise(\"FINANCIAL_CALCULATION_ERROR\")) \\\n",
    "    .withColumn(\"quarantined_at\", F.current_timestamp())\n",
    "\n",
    "# Filter Clean Records\n",
    "df_clean = df_casted.filter(valid_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4f94049-8e47-445a-8bdc-bbafad6a4d7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##5. Deduplication & Final Transformation\n",
    "We use a **Window function** to ensure each transaction_id is unique in the Silver layer, keeping only the most recent version of a transaction based on its load timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4c0738a-1f70-4f1a-b624-e690d40bfa70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 4. DEDUPLICATION & NORMALIZATION ---\n",
    "# Logic: Keep the latest record per transaction_id using a descending row rank\n",
    "window_spec = Window.partitionBy(\"transaction_id\").orderBy(F.col(\"load_dt\").desc())\n",
    "\n",
    "df_silver_final = df_clean.withColumn(\"row_rank\", F.row_number().over(window_spec)) \\\n",
    "    .filter(\"row_rank == 1\").drop(\"row_rank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ede2e39-8f6e-4120-a71f-15e40d2c9477",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6. Atomic Delta Writes & Integrity Constraints\n",
    "Finally, we write the data to Delta Lake and apply storage-level constraints to act as a permanent \"firewall\" against bad data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e4af184-e49d-49e2-ac7f-5c4b5b07cb5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- 5. ATOMIC WRITES ---\n",
    "# Append quarantined records; Overwrite Silver to maintain the current state of truth\n",
    "\n",
    "df_quarantine.write.format(\"delta\").mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(QUARANTINE_TABLE)\n",
    "\n",
    "# Write Silver records\n",
    "df_silver_final.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(SILVER_TABLE)\n",
    "\n",
    "# --- 6. APPLY DELTA CONSTRAINTS ---\n",
    "# Enforce NOT NULL and business logic at the table level\n",
    "spark.sql(f\"ALTER TABLE {SILVER_TABLE} ALTER COLUMN transaction_id SET NOT NULL\")\n",
    "spark.sql(f\"ALTER TABLE {SILVER_TABLE} ALTER COLUMN user_id SET NOT NULL\") # Added constraint\n",
    "\n",
    "try:\n",
    "    spark.sql(f\"ALTER TABLE {SILVER_TABLE} ADD CONSTRAINT positive_total CHECK (final_amount >= 0)\")\n",
    "except Exception as e:\n",
    "    print(f\"Constraint positive_total already exists or could not be added: {e}\")\n",
    "\n",
    "# --- 8. DQ SUMMARY REPORT ---\n",
    "print(\"-\" * 30)\n",
    "print(f\"SILVER LOAD SUMMARY\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Total Bronze Records:     {df_bronze.count()}\")\n",
    "print(f\"Clean Silver Records:    {df_silver_final.count()}\")\n",
    "print(f\"Quarantined Records:     {df_quarantine.count()}\")\n",
    "print(f\"\"\"Null User IDs Caught: {df_quarantine.filter(\"quarantine_reason == 'NULL_USER_ID'\").count()}\"\"\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc0f8432-6d48-48ae-a569-0804925c9521",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Compare revenue between two versions\n",
    "SELECT 'v1' as version, sum(final_amount) FROM `vstone-catalog`.silver_schema.silver_transactions VERSION AS OF 1\n",
    "UNION ALL\n",
    "SELECT 'v2' as version, sum(final_amount) FROM `vstone-catalog`.silver_schema.silver_transactions VERSION AS OF 2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84aa40ff-5c07-458b-a443-e948bfff7945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Industry Logics & Standards \n",
    "**1. Transaction-Level Deduplication**\n",
    "In high-volume retail, the same transaction might be sent twice due to network retries. Our logic partitionBy(\"transaction_id\").orderBy(desc(\"load_dt\")) ensures that the finance team never double-counts revenue.\n",
    "\n",
    "**2. Schema Evolution vs. Integrity**\n",
    "By using .option(\"overwriteSchema\", \"true\"), we allow for technical flexibility, but the Delta Constraints (SET NOT NULL) ensure that this flexibility never compromises the core data quality required for auditing.\n",
    "\n",
    "**3. Idempotency**\n",
    "This notebook is idempotent. Because it overwrites the Silver table with a fresh calculation of the latest truth from Bronze, it can be safely re-run without creating duplicate data.\n",
    "\n",
    "**4. Data Governance (Quarantine)**\n",
    "The quarantine table serves as a \"Data Quality Dashboard\" source. Instead of data simply disappearing, it is stored with metadata (quarantine_reason) so engineers can trace errors back to the source system."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Transactions Silver Table Creation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
