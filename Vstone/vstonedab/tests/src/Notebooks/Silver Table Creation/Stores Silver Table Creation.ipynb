{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39cae16c-c190-4d69-a187-53c7c37bf9b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Silver Layer: Stores Data Transformation\n",
    "**Notebook Objective:** \n",
    "\n",
    "This notebook implements a robust ETL pipeline to transform raw \"Store\" data from the Bronze layer into a validated, deduplicated, and geographically accurate Silver table. \n",
    "\n",
    "It ensures that only stores with valid IDs and realistic GPS coordinates are made available for downstream analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e697a058-65e3-4968-b3a4-faec427c2622",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1. Initial Data Profiling & Geospacial Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad4651b2-976a-4b87-b731-c6727da00f98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this preliminary step, we perform a \"health check\" on the raw data. Since store data is highly dependent on location, we verify that latitude and longitude values fall within global standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3f60a61-94a3-4d53-9796-26d56b208959",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 1. Check for coordinate validity (lat: -90 to 90, long: -180 to 180)\n",
    "-- This helps identify 'impossible' locations before they reach the Silver layer.\n",
    "SELECT \n",
    "  count(*) FILTER (WHERE latitude < -90 OR latitude > 90) as invalid_lat,\n",
    "  count(*) FILTER (WHERE longitude < -180 OR longitude > 180) as invalid_long\n",
    "FROM `vstone-catalog`.bronze_schema.bronze_stores;\n",
    "\n",
    "-- 2. Check for missing store identifiers\n",
    "-- Primary keys are non-negotiable for the Silver layer.\n",
    "SELECT count(*) FROM `vstone-catalog`.bronze_schema.bronze_stores WHERE store_id IS NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0ed2543-fb06-4a93-91fc-1c44d478a517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Environment Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51e6ba5c-482e-4217-84b5-d6a7e1e918b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We define the paths using Unity Catalog's three-tier namespace. We use backticks to handle special characters (hyphens) in the catalog name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4656938a-1d95-4a98-a132-adf7d809ee92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "# Backticks are required because of the hyphen in the catalog name\n",
    "CATALOG = \"`vstone-catalog`\"\n",
    "SILVER_SCHEMA = \"silver_schema\"\n",
    "\n",
    "# Defining table paths for Bronze, Silver, and the Quarantine audit log.\n",
    "BRONZE_TABLE = f\"{CATALOG}.bronze_schema.bronze_stores\"\n",
    "SILVER_TABLE = f\"{CATALOG}.{SILVER_SCHEMA}.silver_stores\"\n",
    "QUARANTINE_TABLE = f\"{CATALOG}.{SILVER_SCHEMA}.quarantine_stores\"\n",
    "# Create the schema if it doesn't exist to avoid initialization errors.\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SILVER_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a0c19f2-1515-418b-85bd-b374145f8147",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Standardization via Pandas UDF\n",
    "To ensure high performance across the cluster, we use a Pandas UDF. \n",
    "\n",
    "This allows us to use efficient Python libraries (like Pandas) to perform vectorized string operations on Spark DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "467f4fed-0836-440d-9223-043c2eec353e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- 3. PANDAS UDF FOR DATA STANDARDIZATION ---\n",
    "@pandas_udf(StringType())\n",
    "def standardize_text_udf(text_series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Standardizes text to lowercase and removes trailing spaces.\"\"\"\n",
    "    return text_series.str.lower().str.strip()\n",
    "# Load Bronze data and convert headers to snake_case for system compatibility.\n",
    "df_bronze = spark.read.table(BRONZE_TABLE)\n",
    "\n",
    "# Standardize column headers to snake_case\n",
    "standardized_cols = [col.lower().replace(\" \", \"_\") for col in df_bronze.columns]\n",
    "df_standardized = df_bronze.toDF(*standardized_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a70097dd-7249-4489-b29c-1626095917d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##4. Geo-Spatial Quality Gates & Quarantine\n",
    "This is the core \"logic gate\" of the pipeline. Records that fail business rules (missing IDs or incorrect coordinates) are diverted to a Quarantine table rather than being deleted, preserving data for future troubleshooting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fba6fc6a-1ddd-44c3-b50f-0bd2331d9236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- QUALITY GATES & QUARANTINE ---\n",
    "# Logic: Latitude must be between -90/90 and Longitude between -180/180.\n",
    "geo_valid = (F.col(\"latitude\").between(-90, 90)) & (F.col(\"longitude\").between(-180, 180))\n",
    "id_valid = F.col(\"store_id\").isNotNull()\n",
    "\n",
    "valid_mask = geo_valid & id_valid\n",
    "\n",
    "# Isolate malformed records\n",
    "df_quarantine = df_standardized.filter(~valid_mask) \\\n",
    "    .withColumn(\"quarantine_reason\", \n",
    "        F.when(~id_valid, \"MISSING_STORE_ID\")\n",
    "         .otherwise(\"INVALID_GEO_COORDINATES\")) \\\n",
    "    .withColumn(\"quarantined_at\", F.current_timestamp())\n",
    "\n",
    "# Filter clean data\n",
    "df_clean = df_standardized.filter(valid_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d4dd183-ffc3-421c-94f9-38eb921760b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- DEDUPLICATION & NORMALIZATION ---\n",
    "# Partition by store_id and rank by load_dt descending to find the latest record.\n",
    "\n",
    "window_spec = Window.partitionBy(\"store_id\").orderBy(F.col(\"load_dt\").desc())\n",
    "\n",
    "df_silver_final = df_clean.withColumn(\"row_rank\", F.row_number().over(window_spec)) \\\n",
    "    .filter(\"row_rank == 1\") \\\n",
    "    .drop(\"row_rank\") \\\n",
    "    .withColumn(\"state\", F.upper(F.col(\"state\"))) \\\n",
    "    .withColumn(\"load_dt\", F.to_timestamp(F.col(\"load_dt\"))) \\\n",
    "    .withColumn(\"postal_code\", F.col(\"postal_code\").cast(\"bigint\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78d9eb9f-cdea-41b8-a7c4-6dcc207eea85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6. Atomic Delta Writes & Table Constraints\n",
    "We commit the data to Delta Lake and apply hard constraints. These constraints act as a permanent firewall at the storage level, preventing any future invalid data from being inserted into this table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b47583e2-4d9d-44c0-95bb-e6dbd148a902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ---  ATOMIC WRITES ---\n",
    "# Append failed records to Quarantine; Overwrite Silver with the fresh clean set.\n",
    "df_quarantine.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(QUARANTINE_TABLE)\n",
    "\n",
    "# Write Silver (Overwrite)\n",
    "df_silver_final.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(SILVER_TABLE)\n",
    "\n",
    "# --- 8. APPLY DELTA CONSTRAINTS ---\n",
    "# Note: Using backticks inside the SQL string for the table name is handled by the SILVER_TABLE variable\n",
    "spark.sql(f\"ALTER TABLE {SILVER_TABLE} CHANGE COLUMN store_id SET NOT NULL\")\n",
    "\n",
    "try:\n",
    "    spark.sql(f\"ALTER TABLE {SILVER_TABLE} ADD CONSTRAINT valid_latitude CHECK (latitude BETWEEN -90 AND 90)\")\n",
    "    spark.sql(f\"ALTER TABLE {SILVER_TABLE} ADD CONSTRAINT valid_longitude CHECK (longitude BETWEEN -180 AND 180)\")\n",
    "except Exception as e:\n",
    "    print(f\"Constraints already exist or could not be applied: {e}\")\n",
    "\n",
    "print(f\"Silver table {SILVER_TABLE} updated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edfcca5c-3bd3-4b29-b19a-63f422b3fd8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- See who changed the table and when\n",
    "DESCRIBE HISTORY `vstone-catalog`.silver_schema.silver_stores;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9de6703-1e5e-4e2e-99a9-33f3ed50d397",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# -- Restore the table to a previous state if an error occurred\n",
    "# RESTORE TABLE `vstone-catalog`.silver_schema.silver_stores TO VERSION AS OF 2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "619759ad-ea7a-468b-97bc-aa30a52bf2d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Industry Logics & Standards \n",
    "**1. Geo-Spatial Integrity**\n",
    "In retail analytics, mapping store locations is vital. By implementing BETWEEN -90 AND 90 checks, we ensure that map visualizations in the Gold layer don't break or show stores in the middle of the ocean due to data entry errors.\n",
    "\n",
    "**2. The Power of Delta History**\n",
    "The DESCRIBE HISTORY command used in this notebook is a key feature of Delta Lake. It allows data engineers to see a full audit trail of every change (who, when, what operation). If a bad update occurs, the RESTORE TABLE ... TO VERSION AS OF command allows for instant \"Undo\" functionality.\n",
    "\n",
    "**3. Idempotency and Overwrites**\n",
    "By using .mode(\"overwrite\") with .option(\"overwriteSchema\", \"true\"), the notebook is idempotent. This means you can run it multiple times, and it will always result in the same, clean \"State of Truth\" without duplicating data.\n",
    "\n",
    "**4. Data Type Enforcement**\n",
    "Casting postal_code to bigint and load_dt to timestamp ensures that the data is ready for mathematical operations and time-series analysis in the next stage of the Medallion pipeline."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5480252379363780,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Stores Silver Table Creation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
