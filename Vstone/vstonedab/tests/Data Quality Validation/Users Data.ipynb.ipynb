{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9f079ca-9d86-4018-9243-529f7101eaa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "This notebook establishes a Comprehensive Data Reconciliation and Integrity Framework for user data stored in JSON format. It serves as a final validation gate to ensure that the data ingested via the Auto Loader pipeline perfectly matches the raw source files in both quantity and content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "967d59f2-006a-44ef-9ecc-7226a9cd7cd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1. Audit Configuration & Global Reconciliation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30cb299b-5eea-460e-9c8c-758220049fb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "The first phase of the audit focuses on \"Global Reconciliation,\" which compares total record counts between the landing zone and the Bronze table.\n",
    "\n",
    "####Logic: \n",
    "\n",
    "The script loads all JSON files from the Chunk 3 directory and compares the aggregate count against the users_bronze table. It also monitors for _rescued_data, a special column used by Auto Loader to capture malformed JSON records that failed to parse correctly.\n",
    "\n",
    "####Why this code: \n",
    " \n",
    " Before performing expensive row-level comparisons, a high-level count validation quickly identifies if any files were skipped or if data corruption occurred during the initial ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "249bd9cb-7fa0-4e26-b005-530e293f020e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. Configuration: Target raw files and the Bronze table in Unity Catalog\n",
    "raw_path = \"/Volumes/vstone-catalog/vstone_schema/chunked_data/chunk3/users/*.json\"\n",
    "bronze_table_name = \"`vstone-catalog`.bronze_schema.users_bronze\"\n",
    "\n",
    "# Columns used to verify data consistency\n",
    "hash_cols = [\"user_id\", \"gender\", \"birthdate\", \"registered_at\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0b8cbb5-59f3-49bc-91db-b3f30cabdfee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Load Raw JSON Data for auditing\n",
    "raw_df = spark.read.format(\"json\").load(raw_path) \\\n",
    "    .withColumn(\"source_label\", F.lit(\"chunk3_json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fbcf798-7cdb-49ef-9ac2-d3fce3cb4d32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 3. Load processed Bronze Table\n",
    "bronze_df = spark.table(bronze_table_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71fc8421-7ddf-4a30-a395-00bd92efbb29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 4. Global Reconciliation (Grand Totals)\n",
    "# Logic: Compare the total number of users and check for parse errors\n",
    "raw_total = raw_df.agg(\n",
    "    F.lit(\"Total Users\").alias(\"Scope\"),\n",
    "    F.count(\"*\").alias(\"raw_count\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7088993-f404-4340-8e5e-cce838b52ee1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_val = bronze_df.select(\"source\").first()[0]\n",
    "\n",
    "bronze_total = bronze_df.agg(\n",
    "    F.lit(\"Total Users\").alias(\"Scope\"),\n",
    "    F.count(\"*\").alias(\"bronze_count\"),\n",
    "    F.count(\"_rescued_data\").alias(\"parse_errors\")# Captures malformed records\n",
    ")\n",
    "\n",
    "comparison_df = raw_total.join(bronze_total, \"Scope\") \\\n",
    "    .select(\n",
    "        \"Scope\",\n",
    "        \"raw_count\",\n",
    "        \"bronze_count\",\n",
    "        (F.col(\"raw_count\") - F.col(\"bronze_count\")).alias(\"count_diff\"),\n",
    "        \"parse_errors\"\n",
    "    )\n",
    "\n",
    "print(f\"--- Step 1: Global Reconciliation (Detected Source Label: {source_val}) ---\")\n",
    "comparison_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e07a3026-de9a-46b4-b21d-b2b40bae62c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Row-Level Integrity Audit (MD5 Fingerprinting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcc51440-d4b4-418d-b79e-56398e9d00b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Once the totals are verified, the script performs a deep-dive \"Integrity Audit\" to ensure the actual content of the records was not altered.\n",
    "\n",
    "####Logic: \n",
    "\n",
    "The script uses an MD5 hashing function to create a \"fingerprint\" of the key data columns for every user in both the raw files and the Bronze table. \n",
    "\n",
    "It then performs a left join to find any user whose fingerprint in the Bronze table doesn't match the raw source. \n",
    "\n",
    "####Why this code:\n",
    "\n",
    " Count validation alone cannot detect data \"drift\" (e.g., if a date format changed or a string was truncated). Hashing provides a mathematical guarantee that the data in the database is an exact 1:1 replica of the source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d64ea7ff-7ce0-44d1-87f0-d35872500a03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 5. Row-Level Integrity Audit (MD5 Fingerprint)\n",
    "def add_row_hash(df, cols, prefix=\"\"):\n",
    "    return df.withColumn(f\"{prefix}row_hash\", F.md5(F.concat_ws(\"||\", \n",
    "        *[F.coalesce(F.col(c).cast(\"string\"), F.lit(\"NULL\")) for c in cols]\n",
    "    )))\n",
    "\n",
    "raw_final = add_row_hash(raw_df, hash_cols, \"raw_\")\n",
    "bronze_final = add_row_hash(bronze_df, hash_cols, \"bronze_\")\n",
    "\n",
    "# Verify: Check if every user has an identical twin in Bronze\n",
    "mismatches = raw_final.join(\n",
    "    bronze_final.select(\"user_id\", \"bronze_row_hash\"),\n",
    "    (raw_final.user_id == bronze_final.user_id) & \n",
    "    (raw_final.raw_row_hash == bronze_final.bronze_row_hash),\n",
    "    \"left\"\n",
    ").filter(F.col(\"bronze_row_hash\").isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "679f3ef3-6b44-4aca-ba41-cdd58f8bc7ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 6. Final Verdict\n",
    "mismatch_count = mismatches.count()\n",
    "if mismatch_count == 0:\n",
    "    print(f\"✅ FINAL VERDICT: 100% Data Integrity Verified for {raw_df.count()} users.\")\n",
    "else:\n",
    "    print(f\"❌ ALERT: Found {mismatch_count} discrepancies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "966c7d00-40c5-4191-9de7-d16e5db284c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Audit Summary\n",
    "\n",
    "\n",
    "* Reconciliation Scope: 2,196,257 total users.\n",
    "\n",
    "* Parse Success: 0 parse errors detected via _rescued_data.\n",
    "\n",
    "* Integrity Status: 100% data fidelity verified across all user records."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Users Data.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
