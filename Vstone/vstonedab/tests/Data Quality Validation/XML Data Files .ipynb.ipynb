{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5422055-f2a1-4d7c-af00-ee1d3f50ff7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook establishes a Data Auditing and Integrity Framework for master data stored in XML format. It ensures that data loaded into the Bronze layer of the Medallion architecture remains consistent and accurate compared to the raw source files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ead0e45f-d6e8-47e3-88ce-a791f3bf3622",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1. Audit Configuration & Hashing Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4195ee3d-b9fc-4b23-b765-e1f160b97ff0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This section defines the mapping between raw XML files and their corresponding Delta tables, alongside a robust method for comparing data content.\n",
    "\n",
    "####Logic: \n",
    "\n",
    "A master_tables dictionary stores metadata for menu_items, payment_methods, stores, and vouchers. It includes specific \"hash columns\" used to generate an MD5 fingerprint for every row.\n",
    "\n",
    "####Why this code: \n",
    " \n",
    " XML parsing can sometimes lead to subtle data type changes. By concatenating key columns and generating an MD5 hash, we create a \"digital signature\" for each record that allows for exact content comparison between the raw file and the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8390cff-447c-4db5-8fb8-65803df58b20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. Configuration: Define the path for Chunk 4 XML files and the target Bronze schema\n",
    "base_path = \"/Volumes/vstone-catalog/vstone_schema/chunked_data/chunk4/\"\n",
    "catalog_schema = \"`vstone-catalog`.bronze_schema\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d33e3457-c4b6-4b60-8ddd-1c8f9231a0c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 2. Metadata Mapping: Link XML files to Bronze tables and define columns for integrity hashing\n",
    "master_tables = {\n",
    "    \"menu_items\": {\n",
    "        \"xml\": \"menu_items.xml\",\n",
    "        \"tag\": \"item\",\n",
    "        \"table\": f\"{catalog_schema}.bronze_menuitems\",\n",
    "        \"hash_cols\": [\"item_id\", \"item_name\", \"category\", \"price\"]\n",
    "    },\n",
    "    \"payment_methods\": {\n",
    "        \"xml\": \"payment_methods.xml\",\n",
    "        \"tag\": \"item\",\n",
    "        \"table\": f\"{catalog_schema}.bronze_paymentmethods\",\n",
    "        \"hash_cols\": [\"method_id\", \"method_name\", \"category\"]\n",
    "    },\n",
    "    \"stores\": {\n",
    "        \"xml\": \"stores.xml\",\n",
    "        \"tag\": \"item\",\n",
    "        \"table\": f\"{catalog_schema}.bronze_stores\",\n",
    "        \"hash_cols\": [\"store_id\", \"store_name\", \"latitude\", \"longitude\"]\n",
    "    },\n",
    "    \"vouchers\": {\n",
    "        \"xml\": \"vouchers.xml\",\n",
    "        \"tag\": \"item\",\n",
    "        \"table\": f\"{catalog_schema}.bronze_vouchers\",\n",
    "        \"hash_cols\": [\"voucher_id\", \"voucher_code\", \"discount_value\"]\n",
    "    }# ... additional tables for stores and vouchers follow the same logic\n",
    "}\n",
    "\n",
    "def add_row_hash(df, cols, prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Logic: Concatenates columns with a separator and generates an MD5 hash.\n",
    "    Why: Provides a unique row-level identifier to verify data hasn't changed during ingestion.\n",
    "    \"\"\"\n",
    "    return df.withColumn(f\"{prefix}row_hash\", F.md5(F.concat_ws(\"||\", \n",
    "        *[F.coalesce(F.col(c).cast(\"string\"), F.lit(\"NULL\")) for c in cols]\n",
    "    )))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bb1c7cb-fdd5-4991-b503-8dc3fae4ec3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Automated Execution Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a354164-e2b6-4f87-940b-e1f6dbdd4552",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This block automates the audit process by iterating through the defined tables and performing row-level reconciliation.\n",
    "\n",
    "####Logic: \n",
    "\n",
    "For each table, the script loads the raw XML and the Delta table. It performs a Global Reconciliation (checking row counts) and a Row-Level Content Check (joining on Primary Keys to compare MD5 hashes). \n",
    "\n",
    "####Why this code: \n",
    "\n",
    "Manual auditing is not scalable. This loop provides an automated \"Success\" or \"Alert\" status for each dataset, ensuring that any discrepancies (missing rows or altered values) are immediately flagged for the engineering team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e32e8770-ab91-4469-88e8-7e221085b533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 3. Execution Loop for Data Auditing\n",
    "for key, info in master_tables.items():\n",
    "    print(f\"\\n--- Auditing Table: {key} ---\")\n",
    "    \n",
    "   # A. Load Raw XML using the spark-xml connector\n",
    "    raw_df = spark.read.format(\"xml\") \\\n",
    "        .option(\"rowTag\", info[\"tag\"]) \\\n",
    "        .load(base_path + info[\"xml\"])\n",
    "    \n",
    "    # B. Load the corresponding Bronze table from Delta Lake\n",
    "    bronze_df = spark.table(info[\"table\"])\n",
    "    \n",
    "    # C. Global Reconciliation: Verify row counts match exactly\n",
    "    raw_count = raw_df.count()\n",
    "    bronze_count = bronze_df.count()\n",
    "    \n",
    "    print(f\"File: {info['xml']} | Raw Count: {raw_count} | Bronze Count: {bronze_count} | Diff: {raw_count - bronze_count}\")\n",
    "    \n",
    "   # D. Integrity Check: Compare MD5 signatures\n",
    "    raw_final = add_row_hash(raw_df, info[\"hash_cols\"], \"raw_\")\n",
    "    bronze_final = add_row_hash(bronze_df, info[\"hash_cols\"], \"bronze_\")\n",
    "    \n",
    "    # Use the first ID column as the join key\n",
    "    pk = info[\"hash_cols\"][0]\n",
    "    \n",
    "    # Join and filter for mismatches or missing records in Bronze\n",
    "    mismatches = raw_final.join(\n",
    "        bronze_final.select(pk, \"bronze_row_hash\"),\n",
    "        on=pk,\n",
    "        how=\"left\"\n",
    "    ).filter((F.col(\"raw_row_hash\") != F.col(\"bronze_row_hash\")) | (F.col(\"bronze_row_hash\").isNull()))\n",
    "    \n",
    "    #output results\n",
    "    if mismatches.count() == 0:\n",
    "        print(f\"✅ SUCCESS: {key} data integrity verified.\")\n",
    "    else:\n",
    "        print(f\"❌ ALERT: Found {mismatches.count()} discrepancies in {key}.\")\n",
    "        mismatches.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "767a8dbe-d739-4aac-8a17-6b75e77bed68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Summary of Integrity Checks\n",
    "\n",
    "* Menu Items: Verified 8 records.\n",
    "\n",
    "* Payment Methods: Verified 5 records.\n",
    "\n",
    "* Stores: Verified 10 records.\n",
    "\n",
    "* Vouchers: Verified 16 records."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "XML Data Files .ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
