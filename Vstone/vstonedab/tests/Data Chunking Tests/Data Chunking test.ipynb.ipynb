{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "704c7b78-6f44-4f44-9afe-67ba6e73b936",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook implements a Validation and Unit Testing Suite for the Data Chunking processes. It ensures that the transformation logic for converting CSV data into JSON and XML formats is accurate and that the Databricks environment maintains proper access to the raw data volumes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7495aa1-93d7-4ce8-8471-33bb4a69dbfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1. Core Transformation Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2134e89c-0f5b-4d48-b3ba-6d0eff156728",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This section isolates the business logic used during the chunking process into testable functions.\n",
    "\n",
    "####Logic:\n",
    "\n",
    " These functions represent the \"unit of work\" for Chunks 3 and 4. The JSON logic ensures the data is coalesced into a single partition for consistent file output, while the XML logic prepares the column structure. \n",
    " \n",
    "####Why this code: \n",
    " \n",
    " Separating logic from the main ingestion scripts allows for unit testing. We can verify that these specific transformations behave as expected without needing to run the entire data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3c3f931-dda9-464d-a780-4540780188be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def transform_csv_to_json_logic(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Business logic for Chunk 3: Preparing data for JSON output.\n",
    "    Logic: Uses coalesce(1) to ensure the output is a single, clean JSON file.\n",
    "    \"\"\"\n",
    "    return df.coalesce(1)\n",
    "\n",
    "def transform_xml_logic(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Business logic for Chunk 4: Preparing data for XML output.\n",
    "    Logic: Selects all columns to ensure the schema is preserved during format conversion.\n",
    "    \"\"\"\n",
    "    return df.select([col(c) for c in df.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9158a71-6818-4047-982d-5f7d6125fa17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Unit and Integration Test Suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a60ac72-8c3b-474c-904e-3ca6253e9693",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This section uses the unittest framework and PySpark's built-in testing utilities to validate the transformation logic and environment connectivity.\n",
    "\n",
    "####Logic:\n",
    "\n",
    "* Schema Validation: Ensures the data structure matches the expected user-defined schema.\n",
    "\n",
    "* Transformation Validation: Uses assertDataFrameEqual to confirm that the transform_csv_to_json_logic function preserves data integrity.\n",
    "\n",
    "* Integration Test: Verifies that the notebook has sufficient permissions to access the raw data Volumes.\n",
    "\n",
    "####Why this code: \n",
    "\n",
    "Manual verification is error-prone. Automated tests provide an immediate \"Safety Net,\" ensuring that any changes made to the transformation functions do not introduce bugs into the production data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20b4d478-e717-44e6-bb28-59f8e0e6117f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "from pyspark.testing.utils import assertDataFrameEqual, assertSchemaEqual\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "class DataChunkingTest(unittest.TestCase):\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        # Access the existing SparkSession in Databricks\n",
    "        cls.spark = spark\n",
    "\n",
    "    def test_schema_equality(self):\n",
    "        \"\"\"Unit Test: Verify the schema matches the expected user structure\"\"\"\n",
    "        data = [(\"1\", \"John Doe\")]\n",
    "        schema = StructType([\n",
    "            StructField(\"id\", StringType(), True),\n",
    "            StructField(\"name\", StringType(), True)\n",
    "        ])\n",
    "        df = self.spark.createDataFrame(data, schema)\n",
    "        \n",
    "       # Define the expected schema for comparison\n",
    "        expected_schema = StructType([\n",
    "            StructField(\"id\", StringType(), True),\n",
    "            StructField(\"name\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        # Logic: Verify that the inferred or defined schema is identical to our requirement\n",
    "        assertSchemaEqual(df.schema, expected_schema)\n",
    "\n",
    "    def test_json_transform_content(self):\n",
    "        \"\"\"Test if transform_csv_to_json_logic preserves data correctly\"\"\"\n",
    "        source_data = [(\"101\", \"Credit\"), (\"102\", \"Cash\")]\n",
    "        source_df = self.spark.createDataFrame(source_data, [\"id\", \"method\"])\n",
    "        \n",
    "        # Apply the transformation function\n",
    "        transformed_df = transform_csv_to_json_logic(source_df)\n",
    "        \n",
    "        # Logic: Confirm that the data values remain unchanged after the transformation\n",
    "        assertDataFrameEqual(transformed_df, source_df)\n",
    "\n",
    "    def test_integration_path_exists(self):\n",
    "        \"\"\"Integration test: Check if the raw volumes are accessible\"\"\"\n",
    "        try:\n",
    "            dbutils.fs.ls(\"/Volumes/vstone-catalog/vstone_schema/raw_data\")\n",
    "            path_exists = True\n",
    "        except:\n",
    "            path_exists = False\n",
    "        \n",
    "        # Logic: This ensures the ingestion pipeline won't fail due to missing path permissions\n",
    "        self.assertTrue(path_exists, \"Raw data volume should be accessible.\")\n",
    "\n",
    "# Prepare the test suite for execution\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(DataChunkingTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91f4d65c-9415-4c3c-9008-9432871ef48b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Execution and Test Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d955cd9b-9284-4ca2-809f-f9005843c1be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The final block executes the tests and generates a formatted report within the Databricks UI.\n",
    "\n",
    "####Logic: \n",
    "\n",
    "It captures the test output into a string buffer and prints a clean summary of the results, including total runs, errors, and failures. \n",
    "\n",
    "####Why this code: \n",
    "\n",
    "Clear reporting is essential for Continuous Integration (CI). This summary allows developers to quickly confirm that the chunking logic is healthy or identify exactly which test failed for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47ae08a8-e639-4d04-8674-431bf15d0bef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from unittest import TextTestRunner\n",
    "\n",
    "# 1. Create a string buffer to catch the report\n",
    "stream = io.StringIO()\n",
    "runner = TextTestRunner(stream=stream, verbosity=2)\n",
    "\n",
    "# 2. Run the tests\n",
    "result = runner.run(suite)\n",
    "\n",
    "# 3. Print the formatted report\n",
    "print(\"======= DATA CHUNKING TEST REPORT =======\")\n",
    "print(stream.getvalue())\n",
    "print(f\"Tests Run: {result.testsRun}\")\n",
    "print(f\"Errors: {len(result.errors)}\")\n",
    "print(f\"Failures: {len(result.failures)}\")\n",
    "print(\"=========================================\")\n",
    "\n",
    "# Debugging Logic: Triggers an alert if any test fails\n",
    "if not result.wasSuccessful():\n",
    "    print(\"\\n[DEBUG MODE] A test failed. You can use %debug in the next cell to investigate.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Chunking test.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
